[{"categories":["event"],"contents":"A-1 セッション 埋め込み表現 A1-1 種差別バイアス やったこと ヒト文 He/She is 〜 who is [MASKED]. モノ文 That/It is 〜 who is [MASKED]. でMASK語予測。（BERT, Roberta-light）\n考察・結論  人間の言語使用がモデルのバイアスに反映される。 非ヒト動物は否定的な感情に関連づけられる。  個人的なコメント  言語モデルの理想のバイアスってなに？人間と一緒でOKなのではないの？  A1-2 知識グラフ埋め込みのPU学習 知識グラフ補完とは Rk(Ei,Ej) Tripleという形で知識の関係を表現。全世界の知識関係を全て網羅することはできないので、補完していく必要がある。\nそのために有効な知識グラフ埋め込み RESCALモデル Positive\u0026amp;Unlabeled学習（抽出型データセットに使う）\n比較 従来手法(RESCAL,logloss) vs 提案手法(RESCAL,squared,PU学習の有無), PUありのRESCALが一番良かった。\nA1-3 単語埋め込み表現のパラメータ削減の研究 単語分散表現の性能向上を低パラメータで狙いたい。\n問題点 パラメタ数が膨大になりやすい。識別精度や表現力を落とすことなくパラメタ数を削減したい。\n提案手法  catとcatsとかはパラメタを共有させる =\u0026gt; 基底ベクトル群（全単語共有）を用意し、行列ベクトル積でなんとかする！（結構精度保つのが難しい） 識別精度に寄与しない非重要後のパラメタ数を削減する =\u0026gt; 損失関数に長さの制約項。Gloveを再構成する事前学習を導入。  結果  小規模コーパスでは向上。大規模では低下。 基底を増やしても変わらない =\u0026gt; 重要語は少ない。 感情語に多くのパラメータが割かれている。  個人的コメント  （QAより）小さいデータセットで提案手法（パラメタ数可変）が素のGloVeより精度がよかった理由として，  タスクに貢献する重要度に長いコードを割り当てた（表現力があがった） 低頻度語に対する正則化   これはC3POの分類に応用可能かもしれない。  A1-4 極座標を用いた階層構造埋め込み 分散表現(Gloveなど)の問題点 訓練時と使用時の空間のギャップをなくし、階層構造を表現したい→極座標を用いる\nPolar Embedding 半径：単語の抽象度、角度：類似度 のパラメータが独立。\n学習方法  半径の最適化：頻度情報や木構造の深さ（階層性がなんであるかはWordNetに従っている。） 角度の最適化：Welsch損失（二乗誤差を使うと極に集中してしまう）一定回数 =\u0026gt; SVGDで一様分布に補正 =\u0026gt; 繰り返す  極にたまってきたら別の極座標に移行して学習することでSVGDの回数を減らしたい 多様体をつぎはぎするイメージで自然にできないか？    結果 単語ペアが上位下位のエッジがあるかというタスクで性能が向上。 ５次元で既存のEuclid空間分散表現より高精度 =\u0026gt; 低次元Euclid空間を有効に活用している。\n個人的な感想、コメント  極座標って一個だけ2πなのは。。。？ 階層が違っても類似度が小さくなるってことか（上位下位関係では制限を付け加えているらしい） 反対語は対角線上に出すことができるのではないか？ 文意の埋め込みはできるのかなぁ\u0026hellip;Euclid空間だと難しそう  B-2 機械学習-2 B2-1Improving Quality of Extractive Summarization with Coverage Analysis 読むのだるい。大事なところをコンピュータで抽出したい。（Summarization Task） Extractive, Abstractive（より大事） ROUGE-NとROUGH-Lという指標。\n前提はBERTSUM  Extractive model Give each sentence a score, and select from the highest 3-gram Blocking Scheme to alleviate repetition PROBLEM: over-focusing on some, not enough  提案手法 Semantic Coverage Analysis を用いる  TF-IDF Semantic Coverage Vector(SCV)[2016] \u0026lt;=CorpusがなかったからGoldSummaryを使う の比較  SCVは改良が怪しい。TFIDFはスコアが少し向上。 QA どうしてうまくいかなかったか？=\u0026gt; Trainingでgold summaryを使ったのがよくなかった。text length が違うから微妙だったのかも.\n個人的コメント やってみた的なやつかな？\nB2-2 Researcher2vec 持橋先生！！！！\n概要 論文の内容から研究者をベクトル化して検索できるサーバを公開した。\n客観的に類似研究が可視化できる。\n背景 CSは論文数が激増してて、死んでいる。既存サービスはメタ情報を見てて論文内容をみてないせいで、微妙\nLDA（確率的潜在意味解析） 各文書に対しトピック分布を推定\n問題点  細かい違いは識別できない（経済学者だけど解析学vsゲーム理論)を分布で捉えるのは難しい。 数学的には単体上でしか扱えていない。 ニューラル手法は学習が難しい  SVD でもよく考えるとWord2Vecは自己相互情報量行列のSVD行列分解と等価である。\n→\nSVDで簡単に文書ベクトル*単語ベクトル みたいに分解できる。\n圧倒的に速くなった。 検索するのも簡単 線形回帰モデルなので、係数を事前計算できるし、\nもはや掛け算しなくても01疎ベクトルだから係数行列の該当行を取り出すだけ。。。mmapでOK!（メモリすらいらない）\n将来的には arXivやACLanthologyに応用したい。日本学術振興会で同様のシステムを開発したい。\nQA 時系列トピックモデル,埋め込み空間での密度変化をうまく可視化したい。=\u0026gt; いま学振でやっている。\nB2-3 文表現の摂動正規化（PretrainedモデルのDebias手法） BERTとかは巨大コーパスの影響でバイアスがあったりする。=\u0026gt;除去したい。\n提案手法  ある文を、MとUにわけて、Uのほうにだけ疑似尤度の合計をとる。 コーパス全体でのバイアスを計算する。  実験  Debiasが達成されたか? -\u0026gt; された（てか損失関数をそうしてるからそれはそうか） 言語理解は劣化したか(GLUE) -\u0026gt; BERTとDebiasedBERTは9/10のタスクで有意に差がないといえた。  QA Q. debiasされているのに、GLUEが変わらないのが理解できない。なんで？\nGLUEはかるときに、finetuningされてるってこと？\nよくわからない、それってdebiasの意味はなくなってないか\u0026hellip;?\nB2-4 XLNetを用いたセンター試験英語不要文除去問題の解答とその分析 東ロボのプロジェクト。理科大。 不要文除去問題をXLNetがどう説いているのかを知りたい=\u0026gt;いろいろ試して挙動をみてみる。\n（先行研究；BERTよりXLNetの方が精度が良い。）\n特殊トークンに置換/語順並べ替え やってみる\u0026hellip;.みたいな  やってみました、精度下がりました。的な\u0026hellip;?（雑ですいません。あまり面白くなかった。) 人が重要と判断したキーワードを問題文から削除する実験では、正解率の低下がほぼ見られなかった。 正解の尤度が最も低下するようにいくつかの単語を選び、未知語を表す特殊トークンに置き換える実験では、問題文のトピックに沿った少数の語の置き換えによって正解の尤度が急激に減少するケースが観察された。  つまり、名詞を手がかりにしているっぽいっちゃぽい。がはっきりとはわからない（？）\nあまりちゃんとメモってません B−3 機械翻訳 B3-1 Transformerの特徴  Multi-hopなAttention機構 =\u0026gt; Encoderの情報を繰り返し抽出 ペアデータが大量に必要 =\u0026gt;（従来手法）RNNに対して外部記憶融合, Coldfusionとか  TransformerにColdFusionを融合したい  ColdFusionではTransformerのmultihopな構造を明示的に活用できない。 そこで、Softmax（最終層）じゃなく、TransformerDecoderBlockにMulti-HeadAttentionを追加し、そこに外部記憶を入力。それらをゲートでConcatenate.  評価実験  話し言葉書き言葉変換 方言変換 外部言語モデルはGPT  QA さすがに、なんか、とかの文頭ワードが提案手法だと消えやすい。が、規則性があるわけではなく理由の考察が難しい。 ２つのモデルの組み合わせの場合は、どちらのモデルを単語ごとにみているかを分析すると良い。\u0026lt;=たしかになー\nB3-2無関連パラレルコーパスでの英文生成 正しいコーパスなしで平易学習がしたい。アライメントにコストがかかる。\nGANの前提知識がないと厳しい。。。\n方針 難文に平易文の出力の特徴を吸収させたい。\nγモデル(GAN) 難しい文のrepresentationを簡単な文のrepresentationに近づける。 同時に、WassersteinGANがうまくいくかをみてみたい。全体の平易化がうまくいくかどうか。\nσモデル(SIM LOSS) D2とE2の出力をコサイン類似度を大きくするように学習する。\n結果 γモデル(JS)がかなりいい感じ。BLEUをそれなりに保持しつつDIFFが上がってるのがポイント。意味対応ありモデルに近い結果が得られた。文長が従来モデルより一貫して短くなっている。 ただし、PBSMTのような多様性が作れていないのが大きな問題。SARIが弱い。\nまとめ  双対型ネットワーク, JS-GAN, 入出力交代機構の後世でUnTS(SOTA)を上回った JS-GAN \u0026gt; W-GAN（理由がよくわからないがsigmoidによって情報が抽象化されている説。） 意味的類似性が学習の上で大切。  企業情報を考慮したキャッチコピーの自動生成 サイバーエージェント！！！と東工大。こうゆうの好き。めっちゃザ企業って感じの研究。\n企業関連語に焦点を当てる。 関連語をMASKし、別の業種の者に変える（これいいのか？）\nPPLMをベースとする。 BERTマスク語予測を企業-ラベルデータでファインチューニング。\n 語彙の全探索により予測する。 追加学習（BERTのパラメータ再調整）する(マスク言語モデル維持もする。)。文脈を考慮できる。 1と2の組み合わせ。確率の積をスコアとする。最終予測単語はスコア最大のものとする。  実験  元のキャッチコピーの生成。正しい単語を予測できるか？ 異なる企業に対する生成。 SKATデータで評価。 結果はぱっとみ、うまくいっていそう。（うまくいっている例だけをみているかもしれない）  B3-4 Transformerでだじゃれ 東工大の先生。 機械にユーモアを理解させたい。生成したい。\nだじゃれを定義 だじゃれについて真面目に解説していてじわる。 併置型と重畳型がある。英語のpunningは重畳型。\n今回は併置型日本語だじゃれを扱う。 編集距離によって反復度合いを計算する。DPで効率的に計算できる。 評価手法はPRS（音の反復を定量的に評価） 損失関数にKLダイバージェンス損失 まず、PRSが分類タスクに適しているかを確認。=\u0026gt; SVMとまあまあ近い精度に。\n満を辞してだじゃれ生成実験 定量評価はぼちぼち、悪くはない。 定性評価はクラウドソーシングで行った。 検索モデルが当然ですがベスト（人が作ったものだから） MLMはだめだった。RLMがよい。\nたとえば 検索：果報は寝て待て、きゃっほー！ RLM：果報は買うほど。\n=\u0026gt; わずかではあるが人間らしいだじゃれが生成された。\n今後は文としての質をあげ、定量的な面白さを組み込みたい。\nQA  音声学的には、破裂音とか言語的特徴を盛り込むと押韻の類似度がスムーズになるかと思う。 Transformer12層だけども、Crossentropyは収束は40エポックくらいでする\u0026hellip;？  提案手法vsIDF、結果が異なった場合は評価はクラウドソーシングで自然な方を選択してもらう。\n感想 評価がどうしても適当になっちゃうからなぁ。これできてすごい！感が少ない。\nB4-2 生成と分類のマルチタスク学習 対話システムの応答が普遍的でつまらない。感情を理解したい。\nやること：応答生成 + 感情認識 を同時に学習する。\n構造：BARTのアーキテクチャとMT-DNNのアルゴリズムを組み合わせる。 工夫：ステップ数が応答生成\u0026raquo;感情認識のため、調整のため各ロスに重み付け\n感想 スコアは向上していたが、、、十分大きいのか\u0026hellip;？ 発話の感情をより理解しているといえる\u0026hellip;? yeahyeahIknowのテンションが中途半端だった。\nB4-3 逆翻訳とフィルタリングによる疑似対話コーパスの生成とそれを用いた対話システムの学習 BTなので聞いておく。NN対話システムの性能を向上させたい。\nやること：BTを対話モデル\u0026amp;逆対話モデルでやる。フィルタリングも。 モデル：MBART, コーパスはGOLD（対話コーパス）+SILVER（BT）+ filter?\n評価：人手評価 改善点：意味を考慮したフィルタリング\n感想 ネガティブな感想に対する人手評価の信憑性に疑問。\n性能向上してるか\u0026hellip;？\nQA  Webコーパスに応答に適していない文が含まれるのでは？ =\u0026gt;それはそうでは NULL responseの影響がBTで増大してそう =\u0026gt; フィルタリングしてるけど完全一致のみ  B4-4 人狼 背景：翻意誘導対話（スマートスピーカーの営業トーク）\nテーマ：人狼ゲーム\n途中抜け\nP4-13 単語属性変換による自然言語推論データ SNLIのデータ拡張\n含意、矛盾、中立の３種類のラベルがある。これを予測したい\n単語属性変換 father -\u0026gt; genderを変換 -\u0026gt; mother\nfather -\u0026gt; singularを変換 -\u0026gt; fathers 鏡映変換で安定的に変換する。\n変換によるDA 性別、反意変換。含意を矛盾に変更。\n結果は変わらなかった、矛盾のラベルが多くなりすぎてモデルが過学習したのではないか。(accは変わってない)\nQA  オリジナルモデルと、DAモデルで統合結果を見るとあがるのでは？ 仮説文（含意）の方が価値がありそう。 「矛盾」とは何？が大事そう。反意ではなくないか。  感想 これこそBTとかどうなんだろう？\n鏡映変換懐かしい\nA-5 A5-1 VisualMRC NTTだ。画像と文章を入力して、QAタスクを解く。\nモデルの入力 OCRトークンに対し、明示的に意味クラストークンを入力に入れておく\n文書画像埋め込み zk = LN(zk_token + zk_pos + 意味クラス + 座標 + 画像特徴)\n学習はマルチタスク学習、損失=NLLloss + Saliencyloss  NLLloss: 正解文と予測文とのnegentropy SaliencyLoss: 質問文と各OCRとの  結果 A5-2 Longformerを用いたHotpotQA HotpotQAとは ちょっと難しいQAtask。根拠文を２つ(gold文章)抽出し、キーワードを抽出し、回答を出す。\n 文書選択 根拠文予測 回答抽出  手法６こをがんばってためした。（少しずつ入力の仕方が違う） A5-3 Moderate to Answer MRC Exmaples with high variability Easy to Answer Examples \u0026hellip; Qがなくても答えられるA\n前の方に答えが分布しがち\n目的：適した教師データをゲットしたい 。教師データ数を減らしたい。 結果：Moderate to answer examples で訓練したときにEMとF1が良い結果になった?!\nExamples with high variability of confidence に絞ったらもっとよくなった！ 手法：どうやって３種類に分けたか？=\u0026gt; 4パターンでどれか正解だったやつをE,不正解だったやつをH, のこりをMにした。 関連研究：confidence vs 分散 でプロット =\u0026gt; EHMが別れた！\n将来：他の指標評価、ehmのそれぞれの特徴を調べる\nA5-4 オープンドメインQA あらゆる範疇の世界知識を使うQAシステム\n既存手法(Lin2018)の微妙なところ  どのようにして解答可能性を判別するか =\u0026gt; 読解モデルに担わせる 解答可能性とは =\u0026gt; 従来は文章中に正解があるか、今回は文章の読解で正解を導けるか。[NEW]  人間に解答可能性を判断させることでより精度良くしよう！！！ A6 埋め込み表現 A6-1 Cross lingual Transfer Zero-shot CLT alleviates the insufficiency of data\nmultilingual raw corpora -\u0026gt; multilingual model =\u0026gt; task model \u0026lt;= task trainingdataでfinetuning, mBERT\nChallenge: Syntatic differences Liu2020 positional embeddings of mBERT duting the fine-tuning WordDependenciesを抽出したい\n dependency head をBERTに入力するにはどうすればいいか =\u0026gt; adapter 良いモデルというのはfinetuning後にノイズに対応できるもの(permutation detection)\nGOAL: Testing models on tasks with demands for order info POSとNERで実験！\n結論：dependency-basedはいまいち、MTLofpermutationdetectionはドイツ語とかで良さげだった。  歯医者で離脱 A7 A7-1 横井さん。重み付き平均が原点に来るよう移動する。\nA7-2 Transformerの理解 Attentionだけじゃない！！混ぜる、というより残す。\n 注意機構\u0026hellip;周囲のベクトルを混ぜ合わせる（線形和） 残差結合\u0026hellip; 自身の入力ベクトルを残す 層正規化\u0026hellip; 拡大縮小 y = 周りからの影響 + 自分の影響 + 残差結合の影響 + バイアス\nノルムの比を計算。=\u0026gt;層をのぼりながら少しずつ混ぜられていく。既存研究と合致。前半の方が混ぜるのが強め。  示唆  最終層でCLSに情報を集め、NextSentecePredictionを解いている Maskは比較的情報を混ぜがち、MLMを解くため？ 中間層では[SEP]にはほぼ混ぜない =\u0026gt; なにこいつ？  QA  自分に戻ってきてるのでは？一部分 12layerも積み上げると意外と混ぜてるのでは？  A7-3 単語埋め込みの決定的縮約 埋め込みにパラメータが大量に必要泣ける。\n[定番] 基底ベクトル（コードブック）と基底番号のリスト（離散符号） 基底の足し算で表現する。効率よい。 DNNの方法もある[2018]けど、ランダム性があって再現性がない\u0026hellip; そこで、乱数のシード値によらない離散符号を学習済み単語埋め込み(GloVe)から獲得する\nまず決定的アルゴリズムによる離散符号の獲得。 1次元に射影してkmeansでクラスタリング。カテゴリ番号を離散符号とする。その後対応する成分を除去。\nその後、基底ベクトルを学習！ 結果 よかった。\nさらに 機械翻訳モデルの単語埋め込みとも比較（LSTMとTransformer）\nQA  基底ベクトルを学習した方の結果は決定的ではないが、既存手法よりはスコアの分散は小さくなった。 翻訳の方は明らかに優れた結果になってるのはなぜか？  A7-4 上位下位関係を識別したい。 単語分散表現を離散コードに変換するという研究が最近ある（前の発表もそうだった）\n単語分散表現を階層的な離散コードに変換できないか？識別器がなくても先頭桁の比較で推測できるのではないか？\nRQ: 意味的な上位下位関係を保つ表現学習はできるか？ただしペアの集合は教師データに使って良い。 階層コードのお約束 0のあとは0\n先行研究  Order Embeddings\n順序関係を埋め込む Semantic Specialization\n単語間の意味関係をベクトルに反映\n=\u0026gt;いいとこどりしたい。  提案手法  LSTMを用いて先頭桁から順にカテゴリカル分布, 単語ベクトル=\u0026gt;確率=\u0026gt;分布のM次元ベクトル 包含関係の計量は、「sがtの上位である確率」とする =\u0026gt; 概念的には動いてくれそう  まとめ  性能の源泉は、 単語の抽象度をコードの非ゼロ桁数に反映できている  QA  バイアスがないか？ポアンカレembeddingsを最初に使うのは？幅と深さには敏感ではない。上位下位関係はmostfrequencesenseに合致しそう。本当はDAG?\u0026lt;=これ隣接行列を学習すればいいんでは  P7-15 opendomainQAにおけるDPR opendomainQAとは  retriever: 大規模文書集合から検索 reader 検索した文書から質問の解答を推定  DPRを用いたQAとは Dense Passanger Retrieverとは？\n TFIDFとかBM25でretrieveすると文脈をあんまり反映してくれない =\u0026gt; encoderを学習して低次元空間に文書らとQuestionを射影して、近いものをピックアップ。  RQ:Can we train a better dense embedding model using only pairs of questions and passages (or answers) =\u0026gt; our final solution is surprisingly simple: the embedding is optimized for maximizing inner products of the question and relevant passage vectors, with an objective compar- ing all pairs of questions and passages in a batch.\n結果 正解率20%くらいであんまりうまくいっていなかった。\nDPR知らなかったしよくわからなかった\u0026hellip;\n残り（力尽きた）   C8-1 大喜利、NLPというかアンケート結果分析、駄洒落の研究者が興味をもっていた\n  D8-2 ビジュラルグラウンディング、グラウンディング不可能なデータのアノテーションを頑張る。=\u0026gt; (SNSデータとかに対し)抽象名詞とか無い物を不可能と推論、複数の場合は複数丸ごとグラウンディング。\n  D8-4 視覚と言語によるナビゲーション課題（理研AIPさきがけ)\nEmbodiedAI:仮想環境内で包括的にロボットに言動を学習させる\u0026lt;=ここ最近夢物語ではなくなっている。 V\u0026amp;L(言語理解)は難しい、PointGoal(GPS)はほぼすでに解けている。視覚情報（画像）を言語に対応づける方法を提案し、1-TENT可視化によりテキストのどこを利用したかの可視化ができた。 ALFREDは最初に指示文を与え、そのあとに短い指示文を長い化して勝手に動くというシステムが必要になる。 (edited)\n  E9-1 中国語の数量詞分離、言語学寄で難しかった\u0026hellip;\n  E9-2 定理証明、自然言語推論タスクの応用。あまり理解できなかったけど面白かった。記号論理学をNLPで補強する。Readable subgoalが鍵。複雑な推論を処理できていたのがポジティブな結果。専門家のdomain知識をリアルタイムに挿入できる\u0026amp;推論の過程が見える。\n  E9-3 DNNの意味表現の体系性 NLIでは誤答理由がむずかった。semanticparsingなら分析できる。構造が学習データと同じ場合は汎化しやすい関係節などは厳しい。\n  E9-4 DTSの部分体系のための定理自動証明機 RTE。前向き推論(言えることを再帰的に洗い出していく）+後ろ向き推論（木を下から上に伸ばす、ほぼDFS、計算量大丈夫か）\n  ","permalink":"https://stardust-coder.github.io/apblog/blog/post-30/","tags":["nlp"],"title":"NLP2021 2〜4日目！！！"},{"categories":null,"contents":"少し毛色の違う日記。\nErikson だれやねん 自分に欠けているものはアイデンティティっぽいので、調べてみる。\nアイデンティティの提唱者。アメリカの発達心理学者。米国で最も影響力のあった精神分析家の1人らしい。（Wikipediaより） エリクソン提唱の発達課題 13-19才 私は誰か？誰でいられるか？（社会的関係、学業）\n成功すると同一性、失敗すると同一性の拡散\n20-39才 愛することが出来るか？（仕事、恋愛、育児）\n成功すると親密性、失敗すると孤独\nそのほか  [年齢] [成功 vs 失敗] 0-2 基本的信頼感vs不信感 2-4 自律性vs恥 4-5 積極性vs罪悪感 5-12 勤勉性vs劣等感 13-19 同一性vs同一性の拡散 20-39 親密性vs孤独 40-64 生殖vs自己吸収 65- 自己統合vs絶望   自分は 過去受容感:低い\n自己理解:低い\n主体性:少し高い 多様性の尊重:高い\n社会的役割:少し低め\n過去も明日も自分が変わらないという感覚 いろいろな経験を経て自分がわかってきたか？なんとかやっていけるのか？\n=\u0026gt; これはかなり厳しいし、低い近くあるな。経験は豊富かもだけど、普遍的な自分 is どこ？フヘンを学ぶ学科なのに。\n自分が思う自分のイメージ」と「相手から見られている自分のイメージ」が照らしあわされる 自分自身を見つめ直していくことや、友人関係や恋人など、重要な他者からフィードバックをもらう\n=\u0026gt;聞いてみよ。数人しか思い当たらないけど。\n自分らしく他の人と関わる 周りに合わせるだけではなく、自分自身のカラーを出していく\n自分と異なる価値観を持つ相手を理解し、その中で「自分がどのように生きていくか？」を考える 自分と他者の違いを理解し受け入れる。ある程度柔軟に。\n自分らしさを発揮できる仕事や学校での役割、もしくは人生のテーマなどを持てている感覚 自分らしさをどのように社会に役立てるかを考え、具体的な進路や仕事の選択をしていく。\n=\u0026gt; なるほどねー。考える。インターンとか企画提案ゼミとかで役に立てているうちに入るのだろうか。進路は決めなければそろそろ。\nまとめ やっぱりせっかくこの学科だし、「普遍」「不偏」「不変」を大事に、もっと日頃から意識して、生きていきたいなあ。\n","permalink":"https://stardust-coder.github.io/apblog/blog/post-28/","tags":null,"title":"Eriksonメモ"},{"categories":["tutorial"],"contents":"HuggingFace Transformers npakaさんのnote にいろいろ書いてある。\nAutoregressive models  古典的な言語モデルでpretrain 一連のトークンに続くトークンを予測 得意なタスクは「テキスト生成」 GPTたち, Transformer-XL, XLNet  Autoencoding models  入力トークンを破損させ、再構築させることでpretrain 通常分全体の双方向表現を構築 得意なタスクは「テキスト分類」「トークン分類」 BERT系, XLM, ELECTRA, Longformer  Seq2seq models  sequence to sequence の問題に変換する 得意なタスクは「翻訳」「要約」「QA」 Transformer, BART, MarianMT, T5  Multimodal models  テキスト入力を画像などと混合し、与えられたタスクに特化する 「分類」のためにのみ機能 MMBT  Retrieval-based models  学習と推論の間に、文書検索を使用。 Dense Passage Retrieval  質問エンコーダ: 質問をベクトルとしてエンコード。 コンテキストエンコーダ : コンテキストをベクトルとしてエンコード。 リーダー: 検索されたコンテキスト内の質問の答えを、関連性スコアとともに抽出。    BlendorBotで遊んでみる pip install transformers でインストールしておく。\ncannot import name 'BlenderbotTokenizer' from 'transformers' transformersのバージョンが2.9.1だった。\npip install --upgrade transformers バージョンが4.3.3になった。\nうわ、なんか大きめのファイルのダウンロードが始まった🥺 嫌だったのでconda環境を専用に作り直してやりました。 ちなみにPyTorchとTensorflowが必要です。\n","permalink":"https://stardust-coder.github.io/apblog/blog/post-27/","tags":["nlp"],"title":"HuggingFace の Transformers を使う"},{"categories":["lectures","Glp"],"contents":"５日間のオンラインSustainability合宿 with sure-co@Indonesia\nFGD Social entrepreneurship\nEnergy + Waste Group Assignment STEP1 Brainstorming Brainstorm and write out visions as a social entrepreneur on the JamBoard\nGrouping  Providing clean energy Reduce Waste  STEP2 Three Sectors of Current Situation Government Sector\nBusiness Sector\nCommunities\nProblems  Waste to energy investment still expensive The govt. sector and business companies should be first prepared well for waste collection and treatment. Lack of composting and waste separation initiative in the main program of the tourism and residential are  Solutions  Provide extra collection unit and more frequent pickups Incentive policy about reduce price for renewable energ  MyMizu Systems Thinking Contrast to traditional thinking, systems surrounding issues you focus on are interacting\nfinancial Social Enterprise models combine business practices and principles with the passion and compassion required to\nsave people in financially sustainable way.\nHeart and Passion of Activist + the business savvy of a CEO\nWhy companies exist?   SHAREHOLDER THEORY increase its profit\n  STAKEHOLDER THEORY responsible to all stakeholders(suppliers, consumers, employees)\n=\u0026gt; Triple Bottom Line(People, Planet,Profit)\n=\u0026gt; John Eikington 2018 Product Recall\n  ↓\n  CORPORATE SOCIAL RESPONSIBILITY(CSR)\nsharing companies' resources. ex.)Pro Bono, Sharing money etc.\n  CREATING SHARING VALUE(CSV)\nnew products that meet social environmental needs and deliver financial return. Access new markets. Improve the capabilities of suppliers\nex.) Ajinomoto: KOKOPlus, Novartis: Hold training camps of health\n  NEW WAVE B Corp vs Benefit Corp\n","permalink":"https://stardust-coder.github.io/apblog/blog/post-25/","tags":["sustainability"],"title":"GLPフィールドワークがなくなって・・・"},{"categories":["nlp"],"contents":"人生初の論文解説発表〜\nこれから先、輪講もろもろでこうゆうのやってくことになるのかな\u0026hellip;\n英語で準備していたけど直前に急に日本語でいいよということに！\n読んだ論文 Low Resource Text Classification with ULMFit and Backtranslation[2019]\n登場用語解説メモ AWD-LSTM(ASGD Weighte-Dropped LSTM) ・Drop Connect ・Average-SGD(更新時に前回の重みではなく、前のT回分の重みの平均を使う)\nLSTMの定式化 Understanding LSTM Networksという2015年の記事を読んでLSTMをunderstandしていく。\nRNN  Humans thoughts have persistence. Understanding each word is based on that of previous words. RNN is pretty amazing! ex.) predicting the last words of  the clouds are in the sky \u0026lt;= EASY! I grew up in France… I speak fluent French. \u0026lt;= need to go back till France   when the gap like this(long-term dependencies) grows, RNN can\u0026rsquo;t learn to connect the information, first explored in depth by Hochreiter(1991)  LSTM(Long Short Term Memory Networks)  capable of learning long-term dependencies the repeating module  standard RNN \u0026hellip; a single tanh layer LSTM \u0026hellip; 4 interacting in a very special way   CELL STATE (a line running through the top of the diagram) GATES  FORGET GATE ($h_{t-1}$, $x_t$ -\u0026gt; [0,1])    ULMFit テクニック Discriminative fine-tuning\nレイヤーが下位になるほど、一般的なドメイン知識を優先させるために学習率を小さくして、なるべく調整しないようにします。\nSlanted triangular learning rates(ロバスト性を保って転移学習させるための工夫)\n学習率そのものをiterationごとに変えていく方法\nConcat Pooling LSTMでは長期の依存関係を捉えることができるとは言え、最後の時点の隠れ層の値だけだとどうしても過去の情報は曖昧になってしまいます。\nそこで、すべての時点の隠れ層の値を使って、次のレイヤーに渡します。\n$$𝐡𝐜=[𝐡_{T},\\mathrm{maxpool}(H),\\mathrm{meanpool}(H)] $$\nForward and Backward The Bidirectional Language Model\nTest Time Augmentation(TTA) TTA is simply to apply different transformations to test examples. Then feed these different transformed texts to the trained model and (weighted) average the results to get more confident answer.\nKaggle解説記事\nVitrual Adversarial Training(VAT) Purpose : 事後確率の分布を滑らかにすることで汎化性能を上げるテクニック。 Advantages: unlabeledも学習に活用できる。（事後確率同士の距離を損失に、正則化のイメージ） Stepは\n p(y|x)とp(y|x+r)を近づける(ダイバージェンス最小化)ようなlossを考える. (rは微小摂動) 多次元だと難しいので, 最も間違えやすい方向のみで考える(LDS) LDSが二次形式にテイラー近似できる。 Hessian Free（差分法）でヘッシアンを求め、その最大固有ベクトルが求めたいr. べき乗法で最大固有ベクトルを求める.  Logistic Regressionの詳細 ULMFit overweights the last sentence for good reason, validated by Logistic Regression with L1 regularization.\nSentence level Sentiment Regression The regressions in Table 4 try to predict two targets: the document’s label and the model’s prediction, using summary stats of the distribution of sentence-level predictions as inputs. More specifically we fit $$target = W · [X−1, X[0], avg(X), max(X), min(X), len(X))]$$\nwhere X is a vector whose elements are the model predicted sentiment of each sentence. We use L1 regularization to get force irrelevant features coefficients to zero.\n","permalink":"https://stardust-coder.github.io/apblog/blog/post-26/","tags":null,"title":"Low Resource Text Classification with ULMFit and Backtranslation"},{"categories":["math"],"contents":"とある本 を読んでいて見つけた。\n最適化手法のベンチマーク（その提案した手法がどれくらい優れているのかを測る基準）となる有名関数があるらしい。 ググってみるとこれ自体分野として確立していそう、ふむふむ。\nWikipediaに一覧としてまとまっていた。\nTest Functions for optimization\nめちゃあるじゃん、しかも3Dプロットと定義域と最適解付き。\nそれぞれの関数は何が特徴なんだろう。\n連続最適化の研究室進んだら使うことになるのかな〜\n","permalink":"https://stardust-coder.github.io/apblog/blog/post-24/","tags":["optimization"],"title":"最適化手法のベンチマーク"},{"categories":["programming","tutorial"],"contents":"UIView.Animation アニメーション中にブザーを鳴らしたい・・・\nAudioKitを使う。\nanimationの中に書いてもダメ\n結局\nDispatch +0.01s とかにした。スレッドなるものがあるらしい。 同じスレッドで処理されると、アニメーションが終わってから別の処理を呼ぶ（=アニメーション中に再生できない） のであえて別のスレッドにおいて非同期処理する。\nできた。\n","permalink":"https://stardust-coder.github.io/apblog/blog/post-21/","tags":["swift"],"title":"Swiftでアニメーション"},{"categories":["nlp"],"contents":"参考記事 2018年の機械学種\n[上記より引用]\nNLP   ACL 最上位国際会議。3年周期。Student Research Workshopがある。\n  NAACL（なっくる） 北米でのACL系。英語や翻訳といったテーマが強いらしい。\n  EMNLP\n  CoNLL(こぬる) Shared Task？学習系手法がメイン。\n  COLING（こりんぐ） 2年ごと。abstractに第二言語を入れて対訳コーパスを作ろうとしていたらしい。\n  EACL ヨーロッパでのACL系。\n  IJCNLP アジア系。２年ごと。\n  AACL アジアでのACL系。\n  LREC（エルレック） 言語リソース系。言語資源の研究が他の研究を支えている。２年ごと。\n  この辺に通している人はすごい人。\nCV, CG, 画像処理  SIGGRAPH WWW CVPR ICCV  その他情報系  SIGMOD VLDB VLDB Journal* TODS ICDE KDD TOIS* JAIR*  Machine Learning  JMLR*   AI  IJCAI AI*  ","permalink":"https://stardust-coder.github.io/apblog/blog/post-22/","tags":null,"title":"国際会議まとめwikiをつくりたい"},{"categories":["programming"],"contents":"KUSUGURI: A Shared Tactile Interface for Bidirectional Tickling 10年以上前か〜。テーマが面白い！\n日本語に訳すと\nくすぐり：相互くすぐりのための共有触覚インターフェース\nかな\u0026hellip;？\n解説ページ\nMasahiro FURUKAWA 先生\n仕組み 視覚 × 触覚\nてのひらにスマホを乗っける。リアル感を出すためにこのときスマホにはちょうど良いサイズでてのひらの画像を表示しておく。\n視覚ついて もともとくすぐられる仕草を見せられると、実際には触れていなくてもくすぐったく感じるらしい。。。へ〜\nそれを視覚に応用した。\nスマホを指でなぞると、その場所どおりの一位置を伝達し、２台目のスマホに指の映像が流れるというもの。\nこれ、イメージとしてはカメラが上にあって、撮影している感じに見える。でも実際にはカメラはなくて、ディスプレイのタップした位置を取得してその座標を送信して、２台目の画面に指を表示しているみたい。\n触覚について 上記の視覚と連動して、スマホの裏側に取り付けた振動子を 作動している。\nというかリンク内の動画を見るのが早い！！！ 今後はスマホ内部の振動でって言ってるけど、当時はなかったのかな〜わざわざ振動子を取り付けている。\n調べてみたら、TapticEngine（iPhoneでタップとかボタンとかおすとカチって言ってるかのような感覚になるギミック）が搭載されてるのはiPhone7からだったーーー\n参考にして（ぱくって）実装してみた。 まあ単にSwiftとAudioKitやらTapticEngineを触ってみたかっただけなんですけど\u0026hellip;\n環境  iOS14.4 Xcode 12.4 （アプデのためのMac容量解放にn年かかった\u0026hellip;） Swift5.3 iPhone8  手順  てのひらを撮影し、背景として表示。(UIViewを全画面サイズにした。) 適当な背景透過画像（今回はやもりのシルエットにした）を用意する。  をアニメーションで動かす。(UIImageView.animate)   3.に合わせて振動させる（AudioKitのSystemAudio）  感想 微妙すぎる！ ぜんぜんおもんない\u0026hellip;\nなぜに本家とここまで差が出るのか\u0026hellip;\nやっぱり今回省いた「遠隔で連動」ってのが本質的な面白さ説あるな。\nせっかく触覚に興味出てきたんだけどなぁ\n","permalink":"https://stardust-coder.github.io/apblog/blog/post-23/","tags":["swift"],"title":"面白い論文を見つけた"},{"categories":["nlp"],"contents":"SSH鍵とはなんぞや home直下の.sshディレクトリに鍵を保存。 configもいじっておくと楽ちん。\nssh abci でログインできるようになった！！！\n登録する ターミナル 操作が基本！！ sftpコマンド ローカルで作ったコードとかファイルをget,putで移動すれば良いのか。ふむふむ。\nsftp abci で\nsftp\u0026gt;\nていうモードに。ふむふむ。\n!を頭につけたコマンドはローカル側の操作ができるのか〜\nローカルにabci用のフォルダ作ってその中で作業すっかー\nBERTを動かしたい！ 日本語版BERTはhuggingfaceのtransmormersに入っているらしいので、それを触ろう。\npip install transformers でOKなんか。\nエラーが消えません venv で仮想環境を作る. source venv/bin/activate する.\n(venv) [trial]$ cat run.sh.~~~~~ python: error while loading shared libraries: libpython3.8.so.1.0: cannot open shared object file: No such file or directory わからなみ。\n","permalink":"https://stardust-coder.github.io/apblog/blog/post-19/","tags":["abci"],"title":"ABCI 初挑戦"},{"categories":["programming"],"contents":"Version Xcode12.4 Swift5.3.2\nめいいっぱいUIViewを作って、そこにカメラの映像を入力する Main.storyboardでめいいっぱいのUIViewを作り、\n8行目の@IBOutlet と接続する。\nclass ViewController: UIViewController { var mySession : AVCaptureSession! var myDevice : AVCaptureDevice! var myImageOutput : AVCapturePhotoOutput! @IBOutlet weak var myView: UIView! override func viewDidLoad() { super.viewDidLoad() // セッションの作成 mySession = AVCaptureSession() // カメラデバイスの取得 if #available(iOS 10.0, *) { let discoverySession = AVCaptureDevice.DiscoverySession(deviceTypes: [AVCaptureDevice.DeviceType.builtInWideAngleCamera], mediaType: AVMediaType.video, position: .back) for device in discoverySession.devices { myDevice = device } } else { myDevice = AVCaptureDevice.default(for: .video) } myView.backgroundColor = UIColor.clear do { // バックカメラからVideoInputを取得 let videoInput = try AVCaptureDeviceInput.init(device: myDevice) // セッションに追加 mySession.addInput(videoInput) // 画像を表示するレイヤーを生成 let myVideoLayer = AVCaptureVideoPreviewLayer(session: mySession) myVideoLayer.frame = myView.bounds myVideoLayer.videoGravity = AVLayerVideoGravity.resizeAspectFill // Viewに追加 myView.layer.addSublayer(myVideoLayer) mySession.startRunning() } catch { return } } override func didReceiveMemoryWarning() { super.didReceiveMemoryWarning() // Dispose of any resources that can be recreated. } override var shouldAutorotate: Bool { get { return false } } } 結局使わなかったこの機能泣\u0026hellip;\n","permalink":"https://stardust-coder.github.io/apblog/blog/post-20/","tags":["swift"],"title":"カメラを背景画像に"},{"categories":["nlp"],"contents":"文書間距離（類似度） 方針は大きく２つ！\n文書をベクトル化し、ベクトル空間上でcos類似度を計算する  分散表現の加重平均をとる cos類似度 実用的  単語の分散表現をベクトル空間にマッピングし、ベクトル空間上での単語群間の距離を計算  EMD -\u0026gt; WMD -\u0026gt; WRD WMD・・・単語単位で考える。変換するときのコストの総和が高い方が意味が離れているとする。直感的！gensimにメソッドがあるので実装が簡単。 WRD・・・単語の分散表現を超球面上に射影 論文\n\u0026lt;= 発想は1つめのと似てないですか？  ","permalink":"https://stardust-coder.github.io/apblog/blog/post-17/","tags":null,"title":"文章間距離メモ"},{"categories":["nlp"],"contents":"API Google Translate 言わずと知れたグーグル翻訳\nAPI: URLでいけた。\nPythonとかで使える。\npip install googletrans でもちらほら不具合がある模様。\nDeepL API: DeepL Pro\n月額630円 + 2500円/1,000,000文字かぁ\n","permalink":"https://stardust-coder.github.io/apblog/blog/post-18/","tags":null,"title":"自動翻訳メモ"},{"categories":["framework"],"contents":"参考書籍 SPRINT How to Solve Big Problems and Test New Ideas in Just Five Days\n今日はユーザテストの日 5人\n1人1h弱でプロトタイプを体験してもらう ふむふむ\nインタビュアー以外がふせんで洗い出してく。。。？\n人手不足！ というわけでZoomを録画し、後ほど聞き返しながらふせんを整理していく戦略に\nお昼を食べたあと、\nふせんはりはり  ポジティブ ネガティブ ニュートラル インタビューの改善（今回は事後なので不要）  に色分けしてはりはりしていく\n頭すごく痛い 共通点探し（パターン発見） 5人中2,3人に共通していた点をリストアップする。\nスプリントクエスチョンと付き合わせる。\nまとめ作業 かなり手応えのあるアイデアが産めた気がする！ 今後  ユーザテストし直し プロトの洗練 プレゼン \u0026amp; フィードバック  ","permalink":"https://stardust-coder.github.io/apblog/blog/post-16/","tags":["ideathon"],"title":"SPRINT奮闘記4日目"},{"categories":["framework"],"contents":"参考書籍 SPRINT How to Solve Big Problems and Test New Ideas in Just Five Days\n反省会 SPRINT反省点  ソリューションスケッチが採用にいたるくらい具体的な粒度でないといけない ソリューションスケッチの内容をそこから考えているのでは意味がない。SPRINTのスピードでは進まない  Deciderは長期目標に即したものを選ぶ！ ただ選んでなくても、Deciderは絶対 長期目標とスプリントクエスチョン軌道修正 ストーリーボード完成 プロトタイプ制作 パワポとGIFを駆使してプロトタイプを作ることに。\nARと動画がメインであったので、AdobeXDがなかなか難しかった。\n思ったこと  4人はむり、9-16の予定が9-0に 睡眠時間４時間 意思疎通は人数が少ない方がいいが、スプリントは7人くらい欲しい  ","permalink":"https://stardust-coder.github.io/apblog/blog/post-15/","tags":["ideathon"],"title":"SPRINT奮闘記3日目"},{"categories":["math"],"contents":"情報幾何(Information Geometry)について 統計学や情報理論を（微分）幾何学的な視点で捉え直す学問.\n教科書 甘利先生の「情報幾何の新展開」を自主ゼミで通読しています。初学者がとっつきやすいような, なるべく簡潔な説明を書きたい！.\n幾何学構造をどうやって定義する？ はじめのうちは「空間に構造を入れる」とかがいまいちピンと来なかった.\n確率分布族：多様体, 点の近傍：接空間, 接空間での定規：計量（内積）, 距離を測る：ダイバージェンス, 点同士を結ぶ：接続\nnこのパラメタで定まる確率分布全体の集合（確率分布族/統計的モデル）をn次元多様体Sとして扱う.\n一般に,計量と接続を外から与えてやると, 多様体の構造が決定でき, 平坦性が定義できる.\n第１部 割と微分幾何を使わない\n凸関数-\u0026gt;座標系-\u0026gt;ダイバージェンス-\u0026gt;計量の流れ。\n距離尺度  距離尺度  よく登場するものが何種類かあるようのでまとめてみる.  L1 norm  L2 norm KL divergence JS divergence Wasserstein distance 輸送コスト最小化問題の解. 堆積した土を移し替えるイメージらしい. α-divergence: positive measure下で唯一flatでinvariant, \"α=-1でKL, α=1でdual KL, α=0でHellinger\"   （追記中）\n","permalink":"https://stardust-coder.github.io/apblog/blog/post-13/","tags":["information geometry"],"title":"情報幾何の新展開"},{"categories":["math","lectures"],"contents":"数理情報工学特論第一とかいう今年から始まったやつ\n 「意味」とは・言語モデル  【意味】 Harrisの分散意味論 単語の「意味（価値？）」　＝　単語の分布？ 意味の合成性　ベクトルの和 （田2017）||w(st) - 1/2(w(s)+w(t))||^2  n-gram  n-gramを使うと、原文とシャッフルとモンキー列が識別できる。 自然言語では長いn-gramでも冪（順位頻度分布両対数プロットで直線）が成り立つ. ランダムはそれが簡単に消える。（モンキーは特に） →課題はシャッフル列をもっと明確に大元から峻別する 時系列解析, 感覚時系列でtheの系列とかなら明確に違いが出る 「深層学習は愛は語れない」 極値解析・ワイブル分布 強定常・弱定常・エルゴード・カオス・複雑系（定義が曖昧） 自然言語は弱定常ですらない。   Zipf則  面白いなぁ、数理民にとっては教養らしい、NLP100本ノックの第４章 マンデルブロー「情報量あたりのコストを最適化している→冪乗則が現れる」 Fermatの原理的な扱い？ ","permalink":"https://stardust-coder.github.io/apblog/blog/post-14/","tags":["NLP"],"title":"自然言語処理"},{"categories":["math"],"contents":"ネットワーク構造 データの性質を見て考える。\nなにをつかうか？全結合層、CNN、RNN・・・\nパラメータは？CNNならチャネル数、・・・\nテキストならAttention, Transformer・・・\n活性化関数は何使う？sigmoid, ReLU, tanh・・・\n目的関数 cross entropy, MSE, Triplet損失 などなどどれを使おう？\nタスクによって変わってくる。\n最適化手法  SGD（確率的勾配降下法） モメンタム Adam バッチサイズや学習率mエポック数 early stoppingの有無 アニーリング etc.  Tips  バッチ正規化 Dropout Data Augmentation L2正則化 アンサンブル学習 補助タスク etc.  大きな構造はどのタスクでも似ているなあ\n","permalink":"https://stardust-coder.github.io/apblog/blog/post-11/","tags":["deeplearning"],"title":"ニューラルネットワークの構成要素"},{"categories":["math","lectures"],"contents":"最適化はおもしろそう。。。\n完全単模性とは 任意の小行列式が0か-1か1の行列のこと.\n別に正方行列じゃなくてもいい.\n成分は, 0か1か-1となる.\n例) 単位行列,\n$A$が完全単模なら, 転置とか, $A$と単位行列を並べてできる行列も完全単模！\n定理 $A$が完全単模行列のとき, 任意の整数ベクトル$\\boldsymbol{b}$に対して$\\{\\boldsymbol{x}:A\\boldsymbol{x} \\leq \\boldsymbol{b}\\}$が整数多面体となる.  $\\min \\boldsymbol{c}^\\mathrm{T}\\boldsymbol{x}, s.t. A \\boldsymbol{x} \\leq b$ とかは, $\\boldsymbol{b}$がintegralなら最適解もintegral. 整数の縛りを外して考えてやっても, 結局凸多面体の面（単体法で解くなら端点）に最適解が来るので, あれ？整数になってね？というノリかと\n完全双対整数性(TDI) $\\max {\\boldsymbol{c}^\\mathrm{T} \\boldsymbol{x}:A\\boldsymbol{x}\\leq b} = \\min {\\boldsymbol{b}^\\mathrm{T} \\boldsymbol{y}: A^\\mathrm{T} \\boldsymbol{y} = \\boldsymbol{c}, \\boldsymbol{y} \\geq 0}$\nという双対問題において,\n$\\boldsymbol{c}$がintegral $\\Rightarrow$ $\\boldsymbol{y}$もintegral.\nあれ, これは完全単模関係ない\u0026hellip;?\n二部グラフとの関係 グラフ$G = (V,E)$ の接続行列を$A$とする. $A$が完全単模$\\Leftrightarrow$ $G$が二部グラフ  $G = (V,E)$の最大マッチングを求める（婚活パーティ的な）問題は, 以下のように定式化できるらしい. $$maxmize \\ \\boldsymbol{1}^\\mathrm{T} \\boldsymbol{x}$$ $$s.t. A\\boldsymbol{x} \\leq \\boldsymbol{1}, \\ \\boldsymbol{x}の成分は0か1$$\n式の意味は後で書く.\nこれを解く代わりに, 線形緩和問題（最後の線形束縛をなくしちゃう！）を解いても, $G$の最大マッチングが求められるということだった. すなわち\u0026hellip;\n\u0026ldquo;主問題の整数最適解\u0026rdquo;-\u0026gt; $G$の最大マッチング\n= （このイコールが完全単模の性質）\n\u0026ldquo;主問題（緩和）の最適解\u0026rdquo; = （このイコールは双対問題の性質）\n\u0026ldquo;(緩和）の双対問題の整数最適解\u0026rdquo;-\u0026gt; 最小頂点被覆\nという関係から有名なKonigの定理が導かれる！！！\n【Konigの定理】二部グラフの最大マッチングと最小頂点被覆の大きさが一致する！  基礎数理でやりましたね、そういえば. 単模, 変換で出てこなくてじれったい\u0026hellip;\n","permalink":"https://stardust-coder.github.io/apblog/blog/post-10/","tags":["optimization"],"title":"完全単模"},{"categories":["math"],"contents":"幾何数理の期末勉強をはじめなきゃ汗\n連結閉曲面の分類定理 連結な閉曲面は\n 穴がkこの浮き輪 射影平面をk\u0026rsquo;こくっつけたもの のどちらかと同相である！！！  同相・・・位相同型。全単射で逆写像も連続である同相写像が存在。 射影平面・・・P^2 位相空間が連結・・・開かつ閉な集合が自明なもの(空集合と全体)しか存在しない。\n弧状連結ならば連結。逆は一般には成り立たない。$\\mathbb{R}^n$上の開集合Aについては同値。 証明は、任意の点$x \\in A$からパスをひいて行ける点全体を$O$として、$O$の範囲を開球をつかってちまちま広げていく感じ。弧状連結を示したいってことは、$A=O\u0026quot;\u0008$を言う、つまりどの点にもパスで行けるよーということが言いたい.\n＜使ったこと＞\n 開球は凸集合なので中身はパスで結べる. PastingLenmaで、xと開球をパスで結べる. 開球を用いた開と閉の定義. Oが開集合かつ閉集合かつ非空 $\\Rightarrow$ O=A（全体）  ホモトピー やりたいこと：空間の間のホモトピー同値を、群の間の同型に対応させる。\nホモロジー群 $H_1(G)$ の意味・・・生成元となるサイクルの個数 以下の３つの操作を導入する.\n 辺の反転 辺の細分 点の縮約  これらの操作を施してもホモロジー群は不変.\nブーケ(bouquet)  $S^1 \\wedge S^1 \\cdots \\wedge S^1$  連結グラフとブーケはホモトピー同値（ホモロジー群も一緒） すべての頂点と接続してサイクルを含まない枝数 n − 1 の枝集合のことを，グラフの全域木という. これは可縮な部分複体になる.全域木を1点に縮約すると，全域木に含まれない m − n + 1 本の枝はループになる   ブーケのホモロジー群は、$H_0(G) = \\mathbb{Z}, H_1(G) = \\mathbb{Z}^m$, $m$は花弁の枚数  $H_0(G)$の意味・・・連結成分の個数 1次元複体$K$上に離散的ダイバージェンス、ベクトル場、ポテンシャルの存在 $\\Leftrightarrow H_0(K) \\simeq \\mathbb{Z} \\Leftrightarrow Kは連結グラフ$\nホモロジーの計算の仕方  愚直に計算する スミス標準形を用いる 完全系列を用いる  完全系列とは すべてのホモロジー群が0であるチェイン複体（アーベル群の列ろ準同型写像たちの組）\n $0 \\xrightarrow{f} A \\rightarrow B$ のとき, $f$は単射 $A \\rightarrow B \\xrightarrow{f} 0$ のとき, $f$は全射 $0 \\rightarrow A \\xrightarrow{f} B \\rightarrow 0$ のとき, $f$は全単射（つまり同型写像）  代表的な図形のホモロジー群を計算してみる n次元球体（中身あり）・・・$H_0(\\mathbb{B}^n) \\simeq \\mathbb{Z}, 他は0$１点に変形レトラクトできるので（可縮） n次元球面（中身なし）・・・$H_0(\\mathbb{S}^n) = H_n(\\mathbb{S}^n) \\simeq \\mathbb{Z}$, 他は$0$$\\mathbb{S}^n$を上半球と下半球に分解し, MV完全列を用いる. 詳細は割愛. トーラス体$V$・・・$H_k(\\mathbb{S}^1)$と同型うまく写像を定めてやると$V \\simeq \\mathbb{B^2}\\times \\mathbb{S}^1 \\simeq \\mathbb{S}^1$となるらしい... トーラス面$\\mathbb{T}^2$・・・$H_1(\\mathbb{T}^2) \\simeq \\mathbb{Z} + \\mathbb{Z}, H_0(\\mathbb{T}^2) \\simeq H_2(\\mathbb{T}^2) \\simeq \\mathbb{Z}, 他は0.$うまく写像を定めてやると$V \\simeq \\mathbb{B^2}\\times \\mathbb{S}^1 \\simeq \\mathbb{S}^1$となるらしい... ブーケ・・・$H_0(G) = \\mathbb{Z}, H_1(G) = \\mathbb{Z}^m$それは...そう... 射影平面$\\mathbb{P}^2$・・・$H_0(\\mathbb{P}^2) = \\mathbb{Z}, H_1(G) = \\frac{\\mathbb{Z}}{2\\mathbb{Z}}, 他は0$ メビウスの帯と円盤の分解に対し, MV完全列を用いる. 詳細は割愛.  自由群 定義 集合Xから生成された自由群F(X)は,\n $X \\in F(X)$ 与えられた群$G$と$f:X \\to G$に対して, $f$が準同型$\\hat{f} \\to G$ に一意に拡張される をみたす群である.  ","permalink":"https://stardust-coder.github.io/apblog/blog/post-9/","tags":["topology"],"title":"幾何数理"},{"categories":["tutorial","programming"],"contents":"休日を1日消費してアプリ作りしてみた めちゃ楽しくはないけど、そこそこ意外と楽しい。 良い趣味になるかも。\nやったこと  Flutterのインストール Flutter公式ドキュメントを読む デモアプリ（Startup namer）の作成 Udemy ×1 Udacity ×1  公式ドキュメントが丁寧だから、書いてある通りやったら何も問題がなかった。\nStartup namerアプリの概要  ドキュメントに載っている「最初に作ってみよう！」っていうやつ  Part1とPart2があってどっちもやった。 英単語辞書（english_wordsパッケージ）からランダムに２単語抽出して繋げて表示する機能 気に入った単語はお気に入り保存できる機能  簡単 Swift挫折した人でもいけるかも。最近のツールはプログラミング言語もそうだけど、用意されているものが豊富で凄い気が。（＝高レイヤーは使い方覚えるだけなので、できる人が増えてしまう）\n完璧に理解する気はないけど、簡単なアプリなら作れそう〜\nStateに触れるのはReact1回に次いで2回目だったけど、そのうち使いこなせるようになるんかなー\n","permalink":"https://stardust-coder.github.io/apblog/blog/post-8/","tags":["App","Flutter"],"title":"はじめてのFlutter"},{"categories":["tutorial"],"contents":"はじめてのGoogle Analytics 無料で使えるアクセス解析ツール\n見れるデータ ほぼ全部。代表的なものは\n PV数：ページ表示回数 セッション数：流入〜離脱回数（30分で新セッション開始） 滞在時間：ページに滞在した平均時間  メニューバー ホーム・カスタム・リアルタイム\nユーザ・集客・行動・コンバージョン だれが・どこから来て・どのページをみて・何をしたか\nアトリビューション\nGoogle Analyticsはどうやってアクセスを解析してるの？ HTMLにJavascriptのタグが記述してある。 全ページに計測タグが必要。ないとセッションが途切れて正確な回数がカウントできなくなる。\n流入元判定 Organic Search, Paid Search, Social, Referral, etc. 公式\nGoogle Analytics用パラメータ パラメータとは、URLの末尾についている「？」が入っている文字列\nmedium（メディア） + source（参照元） + campaign（キャンペーン） + term（キーワード） + content（広告のコンテンツ）から成る。\n","permalink":"https://stardust-coder.github.io/apblog/blog/post-6/","tags":["marketing"],"title":" GoogleAnalytics使い方メモ"},{"categories":["lectures","computer science"],"contents":"シリコン神経ネットワークとは 電子回路上でのNN\nニューロモルフィックハードウェア\n工学 + 脳科学 + 医療\nDLとの違い！！！\n 神経スパイクを用いて情報伝達。ローパスフィルタ的なところも忠実に再現。 DLは脳からinspirationはもらって進化はしてきたが、再現しているわけではない  シリコン神経ネットワークAI  脳と「互換性(compatibility)」のあるシステム -\u0026gt; 言語・記号を介さずに脳と直接コミュニケーション DLより短いラーニングプロセス, 低消費電力 壊れても勝手に治るコンピュータ（Adult Neurogenesisに対応） 進化するAI  どういった研究があるか  海馬損傷とかにたいして埋め込むデバイス 脳シミュレータ ハイブリッド  これまでのプロジェクト  SyNAPSE(IBM) BrainScaleS(ヒューマンブレインプロジェクトより出資) SpiNNaker スピナカー：商用化済 BrainBow（ボルドー第一代大ら）：医療等 MLKVTH（東大）  断崖絶壁  1940s-50s シナプスの性質 最近 fMRI, CTなどにより高次処理情報もわかるように Micro Circuitの情報処理がわからない。シナプスが再現で作れても、Micro Circuitの作り方がわからないという。。。 100%再現できるわけはないから必須のところがどこであるかが解明される必要がある \u0026lt;- 鳥のマネをして飛ぶのに、揚力だけ真似ればOKみたいな話かな\u0026hellip;?  ","permalink":"https://stardust-coder.github.io/apblog/blog/post-4/","tags":["neural science"],"title":"シリコンニューラルネットワーク"},{"categories":["lectures","computer science"],"contents":"TSP サンタさん。\nThm. 枝のコストがmetricだと、多項式時間1/2-近似アルゴリズムが存在。 どんなアルゴリズム？\n Gの最小全域木Tの枝を二重にしたグラフG\u0026rsquo;を作る G\u0026rsquo;のオイラー閉路Xを作る. TSPの解は、Xでの順番でGの全点を訪れるツアー 1/2-近似？ OPT \u0026gt;= OPTから枝を１本取り除いた全域木のコスト \u0026gt;= c(T) c(X) = 2c(T) c(C) \u0026lt;= c(X) (metric) c(C) \u0026lt;= 2OPT = 1/(1-1/2) OPT qed.  Thm. 枝のコストがmetricだと、多項式時間1/3-近似アルゴリズムが存在。 【最近！】\nTSPとTSP(D) TSPは最短ツアーを求める問題。 TSP(D)はある整数B以下のツアーがあるか決定する問題。\nTSPとTSP(D)の関係 TSPが解けたら、TSP(D)は解ける。あたりまえ。\n逆は？実は\nTSP(D)を多項式回解けば、TSPの解がもとまる。\n補題：HAMILTON PATH ∝ TSP(D) ハミルトンパス問題のグラフをGとする。 Gの枝(i,j)があるときdij=1, ないときdij=2, として隣接距離（対称）行列Dをつくる。B = n+1とする。\nTSP(D)を解いて、長さn+1以下のツアーがあると判定された -\u0026gt; パスn本で長さn+1だから、長さ2の枝は高々1本 -\u0026gt; そのツアーの中では長さ1のパスをn-1本連続で通っている。 -\u0026gt; それってハミルトンパスになっている。\n・問題の変換はO(logn)\nNP NPは全てSATに帰着できる。\nSATとは 充足判定問題: CNF論理関数が与えられたとき、それが充足可能か判定する。\nある論理関数が充足可能：【定義】ある真理値割当が存在し、論理関数が真になること。\nHAMILTONPATH ∝ SAT 完全性  C-complete:任意のL' in CがLに帰着可能 closed under reductions: L ∝ L' in C PとNPは帰着に関して閉じている。  SATはNP-complete(Cookの定理)  NPの任意の問題は非決定チューリングマシンで多項式時間で解ける。 「非決定チューリングマシンのプログラムが多項式時間p(n)で状態Yで停止」 同値 「論理関数は充足可能」 つまり、NPの任意の問題がSATに帰着可能。 つまり、SATはNP-complete  SAT ∝ HAMILTONPATH ∝ TSP(D) ∝ SAT 全てNP-complete\nBin-Packing問題 前からみてくのが2-近似アルゴリズム\n万能チューリングマシンU U(M;x)=M(x), UはMの動作をまねしてる。\nThe HALTING Problem 言語H: 全てのチューリングマシンと停止する入力を符号化した文字列の集合 Hを認識するMHの存在を仮定して背理法を目指す。 ＜対角線論法で示す＞\n","permalink":"https://stardust-coder.github.io/apblog/blog/post-5/","tags":["algorithm"],"title":"算法数理メモ"},{"categories":["Web Design","programming"],"contents":"参考にしたサイト  \u0026ldquo;Liva-HugoのGithub\u0026rdquo; \u0026ldquo;Qiita「Hugo + GitHub Pages（独自ドメイン適応）でサイトを作成・公開する」\u0026quot; \u0026ldquo;Install Hugo (Extended) Latest With Shell Script For macOS\u0026rdquo;  ","permalink":"https://stardust-coder.github.io/apblog/blog/post-1/","tags":["Hugo"],"title":"Hugoで作ったブログサイトをGithubPagesで公開"},{"categories":["lectures","computer science"],"contents":"学科長の20年前の研究らしい、興味あり！\n接尾辞木 1973年 全ての接尾辞を格納したcompacted trie\ntrie（トライ）全人類知ってるん？  語源は\u0026quot;retrieval\u0026quot; あるノードの配下の全ノードは、自身に対応する文字列に共通するプレフィックス（接頭部）があり、ルート（根）には空の文字列が対応している。 Wikipedia  compacted trieってなんですか なにがコンパクトなんだろう\n接尾辞配列（suffix array) 1993年 接尾辞のポインタを辞書順にソートした配列 検索は二分探索で行える 二分探索木の高さはO(logn)だった mは検索したい文字列の長さ、一回の比較につき長さmの文字列比較なのでO(m)かかる。 -\u0026gt; 計算量はO(mlogn)\n#!/usr/bin/perl -w use strict; my $t = \u0026#34;hogehogehogege\u0026#34;;\t# この中から my @sa = (0..length($t)-1);\t# Suffix Array初期化 ### Suffix Array の作成 @sa = sort {substr($t, $a) cmp substr($t, $b)} @sa; for (0..$#sa) { print \u0026#34;$_ $sa[$_] \u0026#34;,substr($t, $sa[$_]),\u0026#34;\\n\u0026#34;; } #ポイントは、ポインタ（数字）が辞書順にソートされている ### バイナリサーチ my $k = \u0026#34;ppi\u0026#34;; #これがいる場所を探す my ($l, $u) = (0, $#sa); while ($l \u0026lt;= $u) { my $i = int(($l + $u)/2); my $c = $k cmp substr($t, $sa[$i], length($k)); if ($c \u0026gt; 0) { $l = $i + 1; } elsif ($c \u0026lt; 0) { $u = $i - 1; } else { print qq(\u0026#34;$k\u0026#34; is found at $sa[$i]\\n); last; } } 圧縮接尾辞配列(compressed suffix array)  SA の代わりに Φ[i] = SA^-1[SA[i]+1] を格納 先頭の1文字を消しても辞書順は同じ -\u0026gt; これを生かして圧縮する -\u0026gt; すご。あたまよ。  できること lookup(i): SA[i] を返す (O(log n) 時間) inverse(i): SA-1[i] を返す (O(log n) 時間) Φ[i]: SA-1[SA[i]+1] を返す(O(1) 時間) substring(i,l): T[SA[i]..SA[i]+l-1]を返す – O(l) 時間\nしくみはむずいけど、使ってみるか リンク切れてた。。。\n \n","permalink":"https://stardust-coder.github.io/apblog/blog/post-2/","tags":["algorithm"],"title":"学科長の20年前の研究"},{"categories":["Web Design","programming"],"contents":"あけましておめでとう！ 2021年を記念にブログをつくってみました。 特に動機とかはないです。つくっただけ！ （Hugoのテンプレートに触ってみたかっただけという\u0026hellip;）\n講義で習ったこと関連のことを書くかもしれません。 内容は応用数学とかです。\nあと自分がネット上で探してもなかなかヒットしなかった情報とかを載せれたらいいかも。\nまあ、実質日記\u0026hellip;笑\n","permalink":"https://stardust-coder.github.io/apblog/blog/firstpost/","tags":["Hugo"],"title":"ブログつくりました"}]