---
title: "NLP2021 2〜4日目！！！"
date: 2021-03-18
draft: false

# post thumb
image: "images/featured-post/post-2.jpg"

# meta description
description: "this is meta description"

# taxonomies
categories:
    - "event"

tags:
    - "nlp"

# post type
type: "post"
---

# A-1 セッション 埋め込み表現
## A1-1 種差別バイアス
### やったこと
ヒト文 He/She is 〜 who is [MASKED].
モノ文 That/It is 〜 who is [MASKED].
でMASK語予測。（BERT, Roberta-light）
### 考察・結論
* 人間の言語使用がモデルのバイアスに反映される。
* 非ヒト動物は否定的な感情に関連づけられる。
### 個人的なコメント
* 言語モデルの理想のバイアスってなに？人間と一緒でOKなのではないの？


## A1-2 知識グラフ埋め込みのPU学習
### 知識グラフ補完とは
Rk(Ei,Ej) Tripleという形で知識の関係を表現。全世界の知識関係を全て網羅することはできないので、補完していく必要がある。
### そのために有効な知識グラフ埋め込み
RESCALモデル 
Positive&Unlabeled学習（抽出型データセットに使う）
### 比較
従来手法(RESCAL,logloss) vs 提案手法(RESCAL,squared,PU学習の有無), PUありのRESCALが一番良かった。

## A1-3 単語埋め込み表現のパラメータ削減の研究
単語分散表現の性能向上を低パラメータで狙いたい。
### 問題点
パラメタ数が膨大になりやすい。識別精度や表現力を落とすことなくパラメタ数を削減したい。
### 提案手法
1. catとcatsとかはパラメタを共有させる
=> 基底ベクトル群（全単語共有）を用意し、行列ベクトル積でなんとかする！（結構精度保つのが難しい）
1. 識別精度に寄与しない非重要後のパラメタ数を削減する
=> 損失関数に長さの制約項。Gloveを再構成する事前学習を導入。
### 結果
* 小規模コーパスでは向上。大規模では低下。
* 基底を増やしても変わらない => 重要語は少ない。
* 感情語に多くのパラメータが割かれている。
### 個人的コメント
* （QAより）小さいデータセットで提案手法（パラメタ数可変）が素のGloVeより精度がよかった理由として，
    * タスクに貢献する重要度に長いコードを割り当てた（表現力があがった）
    * 低頻度語に対する正則化
* これはC3POの分類に応用可能かもしれない。

## A1-4 極座標を用いた階層構造埋め込み
### 分散表現(Gloveなど)の問題点
訓練時と使用時の空間のギャップをなくし、階層構造を表現したい→極座標を用いる
### Polar Embedding
半径：単語の抽象度、角度：類似度 のパラメータが独立。
#### 学習方法
* 半径の最適化：頻度情報や木構造の深さ（階層性がなんであるかはWordNetに従っている。）
* 角度の最適化：Welsch損失（二乗誤差を使うと極に集中してしまう）一定回数 => SVGDで一様分布に補正 => 繰り返す
    * 極にたまってきたら別の極座標に移行して学習することでSVGDの回数を減らしたい
    * 多様体をつぎはぎするイメージで自然にできないか？
### 結果
単語ペアが上位下位のエッジがあるかというタスクで性能が向上。
５次元で既存のEuclid空間分散表現より高精度 => 低次元Euclid空間を有効に活用している。
### 個人的な感想、コメント
* 極座標って一個だけ2πなのは。。。？  
* 階層が違っても類似度が小さくなるってことか（上位下位関係では制限を付け加えているらしい）
* 反対語は対角線上に出すことができるのではないか？
* 文意の埋め込みはできるのかなぁ...Euclid空間だと難しそう

# B-2 機械学習-2
## B2-1Improving Quality of Extractive Summarization with Coverage Analysis
読むのだるい。大事なところをコンピュータで抽出したい。（Summarization Task）
Extractive, Abstractive（より大事）
ROUGE-NとROUGH-Lという指標。  
### 前提はBERTSUM
* Extractive model
* Give each sentence a score, and select from the highest
* 3-gram Blocking Scheme to alleviate repetition
* PROBLEM: over-focusing on some, not enough
### 提案手法 Semantic Coverage Analysis を用いる
1. TF-IDF
1. Semantic Coverage Vector(SCV)[2016]  <=CorpusがなかったからGoldSummaryを使う
の比較  
### SCVは改良が怪しい。TFIDFはスコアが少し向上。
### QA
どうしてうまくいかなかったか？=> Trainingでgold summaryを使ったのがよくなかった。text length が違うから微妙だったのかも. 
### 個人的コメント
やってみた的なやつかな？

## B2-2 Researcher2vec
持橋先生！！！！
### 概要
論文の内容から研究者をベクトル化して検索できるサーバを公開した。  
客観的に類似研究が可視化できる。
### 背景
CSは論文数が激増してて、死んでいる。既存サービスはメタ情報を見てて論文内容をみてないせいで、微妙
### LDA（確率的潜在意味解析）
各文書に対しトピック分布を推定  
#### 問題点
* 細かい違いは識別できない（経済学者だけど解析学vsゲーム理論)を分布で捉えるのは難しい。  
* 数学的には単体上でしか扱えていない。
* ニューラル手法は学習が難しい
### SVD 
でもよく考えるとWord2Vecは自己相互情報量行列のSVD行列分解と等価である。  
→  
SVDで簡単に文書ベクトル*単語ベクトル みたいに分解できる。
#### 圧倒的に速くなった。

### 検索するのも簡単
線形回帰モデルなので、係数を事前計算できるし、  
もはや掛け算しなくても01疎ベクトルだから係数行列の該当行を取り出すだけ。。。mmapでOK!（メモリすらいらない）
### 将来的には
arXivやACLanthologyに応用したい。日本学術振興会で同様のシステムを開発したい。
### QA
時系列トピックモデル,埋め込み空間での密度変化をうまく可視化したい。=> いま学振でやっている。

## B2-3 文表現の摂動正規化（PretrainedモデルのDebias手法）
BERTとかは巨大コーパスの影響でバイアスがあったりする。=>除去したい。
### 提案手法
* ある文を、MとUにわけて、Uのほうにだけ疑似尤度の合計をとる。
* コーパス全体でのバイアスを計算する。
### 実験
* Debiasが達成されたか? -> された（てか損失関数をそうしてるからそれはそうか）
* 言語理解は劣化したか(GLUE) -> BERTとDebiasedBERTは9/10のタスクで有意に差がないといえた。
### QA
Q. debiasされているのに、GLUEが変わらないのが理解できない。なんで？  
GLUEはかるときに、finetuningされてるってこと？  
よくわからない、それってdebiasの意味はなくなってないか...?

## B2-4 XLNetを用いたセンター試験英語不要文除去問題の解答とその分析
東ロボのプロジェクト。理科大。
不要文除去問題をXLNetがどう説いているのかを知りたい=>いろいろ試して挙動をみてみる。  
（先行研究；BERTよりXLNetの方が精度が良い。）
### 特殊トークンに置換/語順並べ替え やってみる....みたいな
* やってみました、精度下がりました。的な...?（雑ですいません。あまり面白くなかった。)
* 人が重要と判断したキーワードを問題文から削除する実験では、正解率の低下がほぼ見られなかった。
* 正解の尤度が最も低下するようにいくつかの単語を選び、未知語を表す特殊トークンに置き換える実験では、問題文のトピックに沿った少数の語の置き換えによって正解の尤度が急激に減少するケースが観察された。

つまり、名詞を手がかりにしているっぽいっちゃぽい。がはっきりとはわからない（？）
### あまりちゃんとメモってません


# B−3 機械翻訳
## B3-1
### Transformerの特徴
* Multi-hopなAttention機構 => Encoderの情報を繰り返し抽出
* ペアデータが大量に必要 =>（従来手法）RNNに対して外部記憶融合, Coldfusionとか
### TransformerにColdFusionを融合したい
* ColdFusionではTransformerのmultihopな構造を明示的に活用できない。
* そこで、Softmax（最終層）じゃなく、TransformerDecoderBlockにMulti-HeadAttentionを追加し、そこに外部記憶を入力。それらをゲートでConcatenate.
### 評価実験
1. 話し言葉書き言葉変換
2. 方言変換
外部言語モデルはGPT
### QA
さすがに、なんか、とかの文頭ワードが提案手法だと消えやすい。が、規則性があるわけではなく理由の考察が難しい。
２つのモデルの組み合わせの場合は、どちらのモデルを単語ごとにみているかを分析すると良い。<=たしかになー

## B3-2無関連パラレルコーパスでの英文生成
正しいコーパスなしで平易学習がしたい。アライメントにコストがかかる。  
GANの前提知識がないと厳しい。。。
### 方針
難文に平易文の出力の特徴を吸収させたい。
### γモデル(GAN)
難しい文のrepresentationを簡単な文のrepresentationに近づける。
同時に、WassersteinGANがうまくいくかをみてみたい。全体の平易化がうまくいくかどうか。
### σモデル(SIM LOSS)
D2とE2の出力をコサイン類似度を大きくするように学習する。
### 結果
γモデル(JS)がかなりいい感じ。BLEUをそれなりに保持しつつDIFFが上がってるのがポイント。意味対応ありモデルに近い結果が得られた。文長が従来モデルより一貫して短くなっている。
ただし、PBSMTのような多様性が作れていないのが大きな問題。SARIが弱い。
### まとめ
* 双対型ネットワーク, JS-GAN, 入出力交代機構の後世でUnTS(SOTA)を上回った
* JS-GAN > W-GAN（理由がよくわからないがsigmoidによって情報が抽象化されている説。）
* 意味的類似性が学習の上で大切。

## 企業情報を考慮したキャッチコピーの自動生成
サイバーエージェント！！！と東工大。こうゆうの好き。めっちゃザ企業って感じの研究。
### 企業関連語に焦点を当てる。
関連語をMASKし、別の業種の者に変える（これいいのか？）  
PPLMをベースとする。
BERTマスク語予測を企業-ラベルデータでファインチューニング。  
1. 語彙の全探索により予測する。
1. 追加学習（BERTのパラメータ再調整）する(マスク言語モデル維持もする。)。文脈を考慮できる。
1. 1と2の組み合わせ。確率の積をスコアとする。最終予測単語はスコア最大のものとする。
### 実験
1. 元のキャッチコピーの生成。正しい単語を予測できるか？
1. 異なる企業に対する生成。
SKATデータで評価。
結果はぱっとみ、うまくいっていそう。（うまくいっている例だけをみているかもしれない）

## B3-4 Transformerでだじゃれ 
東工大の先生。
機械にユーモアを理解させたい。生成したい。
### だじゃれを定義
だじゃれについて真面目に解説していてじわる。
併置型と重畳型がある。英語のpunningは重畳型。

### 今回は併置型日本語だじゃれを扱う。
編集距離によって反復度合いを計算する。DPで効率的に計算できる。
評価手法はPRS（音の反復を定量的に評価）
損失関数にKLダイバージェンス損失
まず、PRSが分類タスクに適しているかを確認。=> SVMとまあまあ近い精度に。

### 満を辞してだじゃれ生成実験
定量評価はぼちぼち、悪くはない。
定性評価はクラウドソーシングで行った。
検索モデルが当然ですがベスト（人が作ったものだから）
MLMはだめだった。RLMがよい。

### たとえば
検索：果報は寝て待て、きゃっほー！
RLM：果報は買うほど。  
=> わずかではあるが人間らしいだじゃれが生成された。

今後は文としての質をあげ、定量的な面白さを組み込みたい。

### QA
* 音声学的には、破裂音とか言語的特徴を盛り込むと押韻の類似度がスムーズになるかと思う。
* Transformer12層だけども、Crossentropyは収束は40エポックくらいでする...？




提案手法vsIDF、結果が異なった場合は評価はクラウドソーシングで自然な方を選択してもらう。
### 感想
評価がどうしても適当になっちゃうからなぁ。これできてすごい！感が少ない。

## B4-2 生成と分類のマルチタスク学習
対話システムの応答が普遍的でつまらない。感情を理解したい。  
やること：応答生成 + 感情認識 を同時に学習する。  
構造：BARTのアーキテクチャとMT-DNNのアルゴリズムを組み合わせる。
工夫：ステップ数が応答生成>>感情認識のため、調整のため各ロスに重み付け
### 感想
スコアは向上していたが、、、十分大きいのか...？
発話の感情をより理解しているといえる...?
yeahyeahIknowのテンションが中途半端だった。

## B4-3 逆翻訳とフィルタリングによる疑似対話コーパスの生成とそれを用いた対話システムの学習
BTなので聞いておく。NN対話システムの性能を向上させたい。  
やること：BTを対話モデル&逆対話モデルでやる。フィルタリングも。
モデル：MBART, コーパスはGOLD（対話コーパス）+SILVER（BT）+ filter?  
評価：人手評価
改善点：意味を考慮したフィルタリング
### 感想
ネガティブな感想に対する人手評価の信憑性に疑問。  
性能向上してるか...？
### QA
* Webコーパスに応答に適していない文が含まれるのでは？  =>それはそうでは  
* NULL responseの影響がBTで増大してそう => フィルタリングしてるけど完全一致のみ

## B4-4 人狼
背景：翻意誘導対話（スマートスピーカーの営業トーク）  
テーマ：人狼ゲーム  
途中抜け

## P4-13 単語属性変換による自然言語推論データ
SNLIのデータ拡張  
含意、矛盾、中立の３種類のラベルがある。これを予測したい
### 単語属性変換
father -> genderを変換 -> mother  
father -> singularを変換 -> fathers
鏡映変換で安定的に変換する。  
### 変換によるDA
性別、反意変換。含意を矛盾に変更。  
結果は変わらなかった、矛盾のラベルが多くなりすぎてモデルが過学習したのではないか。(accは変わってない) 
### QA
* オリジナルモデルと、DAモデルで統合結果を見るとあがるのでは？
* 仮説文（含意）の方が価値がありそう。  
* 「矛盾」とは何？が大事そう。反意ではなくないか。
### 感想
これこそBTとかどうなんだろう？  
鏡映変換懐かしい


# A-5
## A5-1 VisualMRC
NTTだ。画像と文章を入力して、QAタスクを解く。
### モデルの入力
OCRトークンに対し、明示的に意味クラストークンを入力に入れておく
### 文書画像埋め込み
zk = LN(zk_token + zk_pos + 意味クラス + 座標 + 画像特徴)
### 学習はマルチタスク学習、損失=NLLloss + Saliencyloss
* NLLloss: 正解文と予測文とのnegentropy
* SaliencyLoss: 質問文と各OCRとの
### 結果

## A5-2 Longformerを用いたHotpotQA
### HotpotQAとは
ちょっと難しいQAtask。根拠文を２つ(gold文章)抽出し、キーワードを抽出し、回答を出す。
- 文書選択
- 根拠文予測
- 回答抽出
### 手法６こをがんばってためした。（少しずつ入力の仕方が違う）

## A5-3 Moderate to Answer MRC Exmaples with high variability
Easy to Answer Examples ... Qがなくても答えられるA  
前の方に答えが分布しがち  
目的：適した教師データをゲットしたい 。教師データ数を減らしたい。 
結果：Moderate to answer examples で訓練したときにEMとF1が良い結果になった?!  
Examples with high variability of confidence に絞ったらもっとよくなった！
手法：どうやって３種類に分けたか？=> 4パターンでどれか正解だったやつをE,不正解だったやつをH, のこりをMにした。    
関連研究：confidence vs 分散 でプロット => EHMが別れた！  
将来：他の指標評価、ehmのそれぞれの特徴を調べる

## A5-4 オープンドメインQA
あらゆる範疇の世界知識を使うQAシステム
### 既存手法(Lin2018)の微妙なところ
* どのようにして解答可能性を判別するか => 読解モデルに担わせる
* 解答可能性とは => 従来は文章中に正解があるか、今回は文章の読解で正解を導けるか。[NEW]
### 人間に解答可能性を判断させることでより精度良くしよう！！！


# A6 埋め込み表現
## A6-1 Cross lingual Transfer
Zero-shot CLT alleviates the insufficiency of data  
multilingual raw corpora -> multilingual model => task model <= task trainingdataでfinetuning, mBERT
### Challenge: Syntatic differences
Liu2020 positional embeddings of mBERT duting the fine-tuning
WordDependenciesを抽出したい
1. dependency head をBERTに入力するにはどうすればいいか => adapter
2. 良いモデルというのはfinetuning後にノイズに対応できるもの(permutation detection)  
GOAL: Testing models on tasks with demands for order info
POSとNERで実験！  
結論：dependency-basedはいまいち、MTLofpermutationdetectionはドイツ語とかで良さげだった。

# 歯医者で離脱

# A7 
## A7-1 
横井さん。重み付き平均が原点に来るよう移動する。

## A7-2 Transformerの理解
Attentionだけじゃない！！混ぜる、というより残す。  
* 注意機構...周囲のベクトルを混ぜ合わせる（線形和）
* 残差結合... 自身の入力ベクトルを残す
* 層正規化... 拡大縮小
y = 周りからの影響 + 自分の影響 + 残差結合の影響 + バイアス  
ノルムの比を計算。=>層をのぼりながら少しずつ混ぜられていく。既存研究と合致。前半の方が混ぜるのが強め。
### 示唆
* 最終層でCLSに情報を集め、NextSentecePredictionを解いている
* Maskは比較的情報を混ぜがち、MLMを解くため？
* 中間層では[SEP]にはほぼ混ぜない => なにこいつ？
### QA
* 自分に戻ってきてるのでは？一部分
* 12layerも積み上げると意外と混ぜてるのでは？

## A7-3 単語埋め込みの決定的縮約
埋め込みにパラメータが大量に必要泣ける。
### [定番] 基底ベクトル（コードブック）と基底番号のリスト（離散符号）
基底の足し算で表現する。効率よい。
DNNの方法もある[2018]けど、ランダム性があって再現性がない...
そこで、乱数のシード値によらない離散符号を学習済み単語埋め込み(GloVe)から獲得する
### まず決定的アルゴリズムによる離散符号の獲得。
1次元に射影してkmeansでクラスタリング。カテゴリ番号を離散符号とする。その後対応する成分を除去。
### その後、基底ベクトルを学習！
### 結果
よかった。
### さらに
機械翻訳モデルの単語埋め込みとも比較（LSTMとTransformer）
### QA
* 基底ベクトルを学習した方の結果は決定的ではないが、既存手法よりはスコアの分散は小さくなった。
* 翻訳の方は明らかに優れた結果になってるのはなぜか？

## A7-4 
上位下位関係を識別したい。
単語分散表現を離散コードに変換するという研究が最近ある（前の発表もそうだった）  
単語分散表現を階層的な離散コードに変換できないか？識別器がなくても先頭桁の比較で推測できるのではないか？
### RQ: 意味的な上位下位関係を保つ表現学習はできるか？ただしペアの集合は教師データに使って良い。
#### 階層コードのお約束
0のあとは0
### 先行研究
1. Order Embeddings  
順序関係を埋め込む  
1. Semantic Specialization  
単語間の意味関係をベクトルに反映  
=>いいとこどりしたい。
### 提案手法
* LSTMを用いて先頭桁から順にカテゴリカル分布, 単語ベクトル=>確率=>分布のM次元ベクトル
* 包含関係の計量は、「sがtの上位である確率」とする
=> 概念的には動いてくれそう
### まとめ
* 性能の源泉は、
* 単語の抽象度をコードの非ゼロ桁数に反映できている
### QA
* バイアスがないか？ポアンカレembeddingsを最初に使うのは？幅と深さには敏感ではない。上位下位関係はmostfrequencesenseに合致しそう。本当はDAG?<=これ隣接行列を学習すればいいんでは

## P7-15 opendomainQAにおけるDPR
### opendomainQAとは
1. retriever: 大規模文書集合から検索
1. reader 検索した文書から質問の解答を推定
### DPRを用いたQAとは
Dense Passanger Retrieverとは？
- TFIDFとかBM25でretrieveすると文脈をあんまり反映してくれない => encoderを学習して低次元空間に文書らとQuestionを射影して、近いものをピックアップ。

RQ:Can we train a better dense embedding model using only pairs of questions and passages (or answers)
=> our final solution is surprisingly simple: the embedding is optimized for maximizing inner products of the question and relevant passage vectors, with an objective compar- ing all pairs of questions and passages in a batch.

### 結果
正解率20%くらいであんまりうまくいっていなかった。

DPR知らなかったしよくわからなかった...


# 残り（力尽きた）

- C8-1 大喜利、NLPというかアンケート結果分析、駄洒落の研究者が興味をもっていた
- D8-2 ビジュラルグラウンディング、グラウンディング不可能なデータのアノテーションを頑張る。=> (SNSデータとかに対し)抽象名詞とか無い物を不可能と推論、複数の場合は複数丸ごとグラウンディング。
- D8-4 視覚と言語によるナビゲーション課題（理研AIPさきがけ)  
EmbodiedAI:仮想環境内で包括的にロボットに言動を学習させる<=ここ最近夢物語ではなくなっている。
V&L(言語理解)は難しい、PointGoal(GPS)はほぼすでに解けている。視覚情報（画像）を言語に対応づける方法を提案し、1-TENT可視化によりテキストのどこを利用したかの可視化ができた。
ALFREDは最初に指示文を与え、そのあとに短い指示文を長い化して勝手に動くというシステムが必要になる。 (edited) 

- E9-1 中国語の数量詞分離、言語学寄で難しかった...
- E9-2 定理証明、自然言語推論タスクの応用。あまり理解できなかったけど面白かった。記号論理学をNLPで補強する。Readable subgoalが鍵。複雑な推論を処理できていたのがポジティブな結果。専門家のdomain知識をリアルタイムに挿入できる&推論の過程が見える。
- E9-3 DNNの意味表現の体系性
NLIでは誤答理由がむずかった。semanticparsingなら分析できる。構造が学習データと同じ場合は汎化しやすい関係節などは厳しい。
- E9-4 DTSの部分体系のための定理自動証明機
RTE。前向き推論(言えることを再帰的に洗い出していく）+後ろ向き推論（木を下から上に伸ばす、ほぼDFS、計算量大丈夫か）