---
title: "NLP2021 開幕！！！"
date: 2021-03-15
draft: true

# post thumb
image: "images/featured-post/post-2.jpg"

# meta description
description: "this is meta description"

# taxonomies
categories: 

tags:

# post type
type: "post"
---

# NLP2021とは
# チュートリアル1
持橋先生（統計数理研究所, 2回目）
## 教科書
* ガウス過程と機械学習（MLP)をかいつまんだ話。
* GPML 結構難しい。 

## 自然言語処理でも連続値を扱う機会が増えている。
- 回帰問題はモデルがいる。
- 世の中には分布がわからないものもたくさんある。ex.)株価とツイートの感情分析（ACL2018workshop)
- NNに突っ込むだけではあまりにも乏しい=>ガウス過程をちゃんと考える！

## ガウス過程のイメージ
- 関数をランダムに生成する。（関数の確率分布）
- ベイズ的なカーネル法ともいえる。
データが与えられると、関数の事後分布が得られる。
期待値でもとまる。+ データが無いところは分散が大きい（通常の最適化では扱えない分散）

- 文字列、グラフとか木とか確率モデルにも使える、カーネルさえ定義できれば（まだあまり研究されてない）  
注意：急に変化するものには使えない。NNとかと絡める。ただマクロにはなめらかに見える説がある。

## 持橋先生の博士論文
潜在的話題をブラウン運動しているとして追跡する。ParticleFilterで追跡する。区分的にモデル化した。ガウス過程はまだ知られていなかった。

=> 今ならできそう！！ガウス過程でなめらかに表現できそう！いまなら！
（ほんとうはこうなってるはずだというアイデアを大事にしてほしい。5,10年後に解けるようになるかもしれない）

## 線形回帰モデルの復習
丁寧に正規方程式を導出。。。まあ。。。

=>拡張すれば任意の関数が基底関数の線形和で表せるのでは！？  
その一例が
動径基底関数（大量のガウス分布）

## 次元の呪い
基底関数の数（＝パラメータ数）が入力ベクトルの次元に対して指数的に増加して計算が死ぬ。

## ガウス過程でESCAPE!
w〜N(0,αI)とする。  
線形回帰モデル y = Φw 〜 N(0,αΦΦT) <=ガウス過程（どんなXについても成り立つ。N->∞でもoK）
がガウス分布に従う重みwを積分消去している。

ポイント：ガウス過程は「xが似ていればyも似ている」ことを数学的に表している道具。
カーネルは「xとx'の近さ」のイメージ。

## 自然言語処理への応用
入力に対して、連続値を返したり、分散をモデル化したりする！！ガウス過程！！

* 機械翻訳の評価(Cohn2013)
* 作文の自動採点
* 文の処理時間のモデル化

## カーネル法
カーネル関数のハイパーパラメータを学習する。
MCMCとかで探すといいかも。嶺がたくさんあるので。

どういうカーネルを使うかで結構結果が変わる。
いろいろなカーネルを用意して、重み係数を学習するとかもできる。（尤度最大化とかでOK!）

## ガウス分布以外の観測モデル
ポアソン観測モデル・・・λ=exp(f(x)), f(x)がガウス過程に従うとする。  

## NNとの関係性
Neal(1996)
潜在層が増えれば増えるほどCLTで出力はガウス分布に収束する。つまり、NNはガウス過程に近づいてきそう。
=>実験ではH=10ですでにほぼガウス過程に従っている。一緒ならNNの代わりにガウス過程を使った方がよさそう！！

## 教師なし学習だとどうする？
観測値（連続値）のみがわかっている。=>確率的種成分分析(Probabilistic PCA)  
$$y_n = Wx_n + ε $$ 
通常のPCAにノイズを乗っけた拡張。

だけれども

Wが巨大すぎる。期待値とって積分消去できないか？  
=> Gaussian Process Latent Variable Model
非線形圧縮していて見やすい。すばらしい。

## NLPへの応用研究の紹介
* シンタクスは離散だけど、セマンティクスは連続説。

* スペクトル混合カーネル(2013)
ガウス過程で使うカーネルをフーリエ領域で自動学習できる（ほんとかよ！？）
```
ボホナーの定理
任意の定常カーネルは周波数領域に確率密度ぷさいが存在する。
```
周波数領域で混合ガウス分布を考えればよいということ。

<=>

k(t) = spectral mixture kernel
を考えていることに相当。

### 人間の動作からの副詞の理解（家庭用ロボットに有用）
副詞って、動作やろ！動作と副詞を統計的に結び付ける方法として、ガウス過程が使える！
#### 手法
骨人間の関節座標（48dim)の時系列データを副詞に対応させる。
48d => 3d にGPLVMで潜在空間内に圧縮。
#### SMLDA(スペクトル混合潜在ディリクレ配分法)
予測パープレキシティはよくなった。  
動作=>副詞の予測はかなりいい感じ。  
本当は副詞=>動作生成をしたい。今後の課題。

### 地理言語学
フィジー語の方言の分布をモデル化したい。=>ガウス過程でできるのでは？  
softmax(x), xがガウス過程に従うとする。観測値から逆算したい。

楕円スライス？とかいう手法もある。

## まとめ
* 複雑な関数のモデル化（言語分布とか、NNの中とか）はガウス過程の専売特許。
* 連続的に変化する関数を生成する。一番簡単な使い方は、ガウス過程回帰＝カーネル法に基づくベイズ的な非線形回帰モデル
* 教師なし学習も応用可能。
* DNN ノード数→∞ でガウス過程に一致。解析的に行列計算でできる！
* 計算量はナイーブにはO(N^3), 実質O(N^2)くらい。予測はconst.？？？

# チュートリアル2
きたきた、すずきたいじ！！！！！

## DL理論といっても。。。
解釈可能性：X AIとか  
各種テクニックの解析：Dropoutとかもろもろのテクは何をしているのか
深層学習の原理解明：⭐️ここを話す
学習の本質解明：#よい学習 とは

## 理論的課題
* 表現能力（バイアス）
* 汎化能力（バリアンス）
* 最適化能力

予測誤差 <= バイアス+バリアンス  
なので、これらがバランスするのはどこかを考える。

## 近年注目を浴びているスケーリング則(2020)
実は古典的なシンプルなモデルでもこの現象は現れる（特論でやったな）  
予測誤差がC(M/n + M^-α)で抑えられる=>Mについて最小化すると、スケーリング則（loglogで直線）

GPT3とかが本当に実験でこの理論を再現してしまった。

## 表現能力
万能近似能力がある。ReLUとかでも、無限個素子を用意すれば滑らかな関数も表現できる。
### Cybenkoの理論
階段関数の幅をいくらでもつくれる。足し引きするとcosとsinをいくらでもよく近似できる。cos, sinが実現できるならFourier(逆)変換もできる。  
任意の連続関数はFourier変換でcos,sinの足し合わせなので、近似できる。

### Ridgelet変換
非線形ノードの足し合わせで関数を表現できる。（実はwavelet変換とラドン変換）

### 浅いKernel法も万能近似能力はあるヨ
実際は「表現力」が特徴。近似効率、近似精度、etc...

### ランダム特徴で実験！！！
一層目を乱数で固定したモデルを考える。（Kernel法の一例）
Q. ある対象の関数f*を近似するのに必要なランダム特徴量の数Mはどれくらい？？  
A. １つのニューロンを近似するのにも$M = exp(Ω(d))$必要。数打ちゃ方向は当たるが効率が悪すぎる。

### Barron classの近似定理(2018)
NN（一層目もパラメータを動かす）と、次元の呪いを回避できることが示せる。M^-1/dがM^-1になる。

### 数学的に一般化すると
「凸結合をとって崩れる性質をもつ関数の学習はDLが強い」
非凸性、スパース性 でこれまでの理論はかなり語れる。

### Self Attentionの数理表現
Transformer = 相互作用のあるparticle system
* 各単語に相当するparticleが近隣のparticleと相互作用しながら、分布の形が変わっていく。
* 特徴空間における近さを用いる。
* 特徴空間における表現を修正してゆく相互作用システム
* 汎化誤差理論研究はまだ見たことがない。

## 汎化誤差
ResNetとかtwolayerReLUとか、あれ？過学習しない・・・

### １つの解釈
パラメータ数 >> データサイズ >> 「実質的自由度」  
=> ノルム型バウンド、圧縮型バウンド

#### Uniform boundとかNaive boundとか

### Bartlett(2017)
正規化マージンバウンド。重みのノルムしか聞いてこない。層の幅とかは効いていない。

## 最適化理論
情報幾何の最後にやったのと近い。
### NTK
一層目だけを動かす。（非線形モデル）  
実は テイラー近似すると
f_W(x) 〜 (W-W0)∇w f_W0(x)  
初期値のスケールが大きいので初期値周りの線形近似でデータにフィットしてしまう。

Arora et.el(2019) 横幅を大きくすれば勾配法によって大域的最適解へ線形収束し、汎化誤差もバウンドできる。

NTKのグラム行列の固有値分布をみると、固有値が減衰していく。（汎化の目線からも嬉しい）なぜなら  
低周波成分が最初にcaptureされる。その後高周波成分が補足されて、過学習していく。
=>EarlyStoppingとかで過学習を防げる。

### 自然勾配法
NGDは収束がはやい。早めに止める。

## Beyond Kernelの話が必要となってくる。
別の話。
### 平均場解析
パラメータの収束を、分布の収束にすりかえる。
スパースな解：陰的正則化
- NTL, カーネル法:L2正則化
- 平均ば理論:L1正則化

## ノイズあり勾配法(SGD)と大域的最適性
Flat minimum > Sharp minimum が望ましい説。  
SGDはFlatに落ちやすい=>良い汎化性能。と説明される。

### ノイズによる平滑化
解にノイズをのせて期待値をとっていることになる。目的関数の平滑化につながっている。

GLD:勾配法 + ガウシアンノイズ  
定常分布：πはexp(-βL(X))に収束。（βは逆温度）

Teacher-Student Modelへの応用
=> 深層学習が有利！！