---
title: "Low Resource Text Classification with ULMFit and Backtranslation"
date: 2021-03-05
draft: false

# post thumb
image: "images/featured-post/post-2.jpg"

# meta description
description: "this is meta description"

# taxonomies
categories: 

tags:
  - "nlp"

# post type
type: "post"
---

### AWD-LSTM(ASGD Weighte-Dropped LSTM)
ãƒ»Drop Connect
ãƒ»Average-SGD(æ›´æ–°æ™‚ã«å‰å›ã®é‡ã¿ã§ã¯ãªãã€å‰ã®Tå›åˆ†ã®é‡ã¿ã®å¹³å‡ã‚’ä½¿ã†)

#### LSTMã®å®šå¼åŒ–
[Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)ã¨ã„ã†2015å¹´ã®è¨˜äº‹ã‚’èª­ã‚“ã§LSTMã‚’understandã—ã¦ã„ãã€‚

##### RNN
* Humans thoughts have persistence. Understanding each word is based on that of previous words.
* RNN is pretty amazing!
* ex.) predicting the last words of 
  - the clouds are in the sky <= EASY!
  - I grew up in Franceâ€¦ I speak fluent French. <= need to go back till France
* when the gap like this(long-term dependencies) grows, RNN can't learn to connect the information, first explored in depth by Hochreiter(1991)

##### LSTM(Long Short Term Memory Networks)
* capable of learning long-term dependencies
* the repeating module
  - standard RNN ... a single tanh layer
  - LSTM ... 4 interacting in a very special way
* CELL STATE (a line running through the top of the diagram)
* GATES
  - FORGET GATE ($h_{t-1}$, $x_t$ -> [0,1])




### ULMFit
#### ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯
Discriminative fine-tuning  
ãƒ¬ã‚¤ãƒ¤ãƒ¼ãŒä¸‹ä½ã«ãªã‚‹ã»ã©ã€ä¸€èˆ¬çš„ãªãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã‚’å„ªå…ˆã•ã›ã‚‹ãŸã‚ã«å­¦ç¿’ç‡ã‚’å°ã•ãã—ã¦ã€ãªã‚‹ã¹ãèª¿æ•´ã—ãªã„ã‚ˆã†ã«ã—ã¾ã™ã€‚

Slanted triangular learning rates(ãƒ­ãƒã‚¹ãƒˆæ€§ã‚’ä¿ã£ã¦è»¢ç§»å­¦ç¿’ã•ã›ã‚‹ãŸã‚ã®å·¥å¤«)  
å­¦ç¿’ç‡ãã®ã‚‚ã®ã‚’iterationã”ã¨ã«å¤‰ãˆã¦ã„ãæ–¹æ³•

#### Concat Pooling
LSTMã§ã¯é•·æœŸã®ä¾å­˜é–¢ä¿‚ã‚’æ‰ãˆã‚‹ã“ã¨ãŒã§ãã‚‹ã¨ã¯è¨€ãˆã€æœ€å¾Œã®æ™‚ç‚¹ã®éš ã‚Œå±¤ã®å€¤ã ã‘ã ã¨ã©ã†ã—ã¦ã‚‚éå»ã®æƒ…å ±ã¯æ›–æ˜§ã«ãªã£ã¦ã—ã¾ã„ã¾ã™ã€‚

ãã“ã§ã€ã™ã¹ã¦ã®æ™‚ç‚¹ã®éš ã‚Œå±¤ã®å€¤ã‚’ä½¿ã£ã¦ã€æ¬¡ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã«æ¸¡ã—ã¾ã™ã€‚

$$ğ¡ğœ=[ğ¡_{T},\mathrm{maxpool}(H),\mathrm{meanpool}(H)] $$


#### Forward and Backward

[The Bidirectional Language Model](https://medium.com/@plusepsilon/the-bidirectional-language-model-1f3961d1fb27)


### Test Time Augmentation(TTA)
TTA is simply to apply different transformations to test examples. Then feed these different transformed texts to the trained model and (weighted) average the results to get more confident answer. 

[Kaggleè§£èª¬è¨˜äº‹](https://www.kaggle.com/andrewkh/test-time-augmentation-tta-worth-it)

### Vitrual Adversarial Training(VAT)
Purpose : äº‹å¾Œç¢ºç‡ã®åˆ†å¸ƒã‚’æ»‘ã‚‰ã‹ã«ã™ã‚‹ã“ã¨ã§æ±åŒ–æ€§èƒ½ã‚’ä¸Šã’ã‚‹ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã€‚
Advantages: unlabeledã‚‚å­¦ç¿’ã«æ´»ç”¨ã§ãã‚‹ã€‚ï¼ˆäº‹å¾Œç¢ºç‡åŒå£«ã®è·é›¢ã‚’æå¤±ã«ã€æ­£å‰‡åŒ–ã®ã‚¤ãƒ¡ãƒ¼ã‚¸ï¼‰
Stepã¯
* p(y|x)ã¨p(y|x+r)ã‚’è¿‘ã¥ã‘ã‚‹(ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹æœ€å°åŒ–)ã‚ˆã†ãªlossã‚’è€ƒãˆã‚‹. (rã¯å¾®å°æ‘‚å‹•)
* å¤šæ¬¡å…ƒã ã¨é›£ã—ã„ã®ã§, æœ€ã‚‚é–“é•ãˆã‚„ã™ã„æ–¹å‘ã®ã¿ã§è€ƒãˆã‚‹(LDS)
* LDSãŒäºŒæ¬¡å½¢å¼ã«ãƒ†ã‚¤ãƒ©ãƒ¼è¿‘ä¼¼ã§ãã‚‹ã€‚
* Hessian Freeï¼ˆå·®åˆ†æ³•ï¼‰ã§ãƒ˜ãƒƒã‚·ã‚¢ãƒ³ã‚’æ±‚ã‚ã€ãã®æœ€å¤§å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ãŒæ±‚ã‚ãŸã„r.
* ã¹ãä¹—æ³•ã§æœ€å¤§å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ã‚’æ±‚ã‚ã‚‹.

### Logistic Regressionã®è©³ç´°
ULMFit overweights the last sentence for good reason, validated by Logistic Regression with L1 regularization.

#### Sentence level Sentiment Regression

The regressions in Table 4 try to predict two targets: the documentâ€™s label and the modelâ€™s prediction, using summary stats of the distribution of sentence-level predictions as inputs. More specifically we fit
$$target = W Â· [Xâˆ’1, X[0], avg(X), max(X), min(X), len(X))]$$

where X is a vector whose elements are the model predicted sentiment of each sentence. We use L1
regularization to get force irrelevant features coefficients to zero.